
<!DOCTYPE html>


<html lang="en" data-content_root="../../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Support Vector Machine (SVM) &#8212; Dr.Hadi Sadoghi Yazdi</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="../../../_static/styles/bootstrap.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />

  
  <link href="../../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=3ee479438cf8b5e0d341" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-design.min.css?v=87e54e7c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341" />
<link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341" />
  <script src="../../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=3ee479438cf8b5e0d341"></script>

    <script src="../../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'courses/PR/Classification/SVM1';</script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Fihser Classifier" href="Fisherclassifier.html" />
    <link rel="prev" title="Support Vector Data Description (SVDD)" href="SVDD.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../../Home_Page.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../../_static/Hadi_Sadoghi_Yazdi.png" class="logo__image only-light" alt="Dr.Hadi Sadoghi Yazdi - Home"/>
    <script>document.write(`<img src="../../../_static/Hadi_Sadoghi_Yazdi.png" class="logo__image only-dark" alt="Dr.Hadi Sadoghi Yazdi - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../Home_Page.html">
                    Welcome to my personal Website!
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../../../Courses.html">Courses</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 current active has-children"><a class="reference internal" href="../../pattern_recognition.html">Pattern Recognition</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l3 has-children"><a class="reference internal" href="../Introduction/PR_intro.html">Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../Introduction/DataSet.html">Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Introduction/Model.html">Model</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Introduction/Cost.html">Cost_Function</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Introduction/LearningRule.html">Learning_Rule</a></li>
</ul>
</details></li>
<li class="toctree-l3"><a class="reference internal" href="../Visualization/PR_intro_Visualization.html">Visualization</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../Clustering/PR_intro_Clustering.html">Clustering Concept</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../Clustering/Clustering_1.html">Clustering</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Clustering/FCM_1.html">k-means and fuzzy-c-means clustering</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Clustering/FCM_Saghi_Project.html">Complementary of FCM</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Clustering/LinkageClustering_1.html">Project Linkage clustering</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Clustering/e_insensitive_linkage.html">Project <span class="math notranslate nohighlight">\( \epsilon \)</span> -insensitive Linkage</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Clustering/SOFM_Project.html">Project Title: Self-Organizing Feature Map (SOFM)</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../Regression/Introduction_Regression.html">Regression</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../Regression/Regression_1.html">Linear Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Regression/NonLinearRegression.html">Non-linear Regression: The starting point</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Regression/Linearization.html">Linearization</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Regression/Kernel_for_Regression.html">Kernel method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Regression/EvaluationModelSelection.html">Project : Evaluation and Model Selection</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Regression/Solution_for_Regression.html">Solution Method</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Regression/TheoryRegression.html">Theoretical Aspects of Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="../Regression/ApplicationsPatternRecognition.html">Applications in Pattern Recognition</a></li>
</ul>
</details></li>
<li class="toctree-l3 current active has-children"><a class="reference internal" href="PR_intro_Classification.html">Classification</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="SVDD.html">Support Vector Data Description (SVDD)</a></li>
<li class="toctree-l4 current active"><a class="current reference internal" href="#">Support Vector Machine (SVM)</a></li>
<li class="toctree-l4"><a class="reference internal" href="Fisherclassifier.html">Fihser Classifier</a></li>
<li class="toctree-l4"><a class="reference internal" href="KernelFisherDiscriminantAnalysis.html">Kernel Fisher Discriminant Analysis</a></li>
<li class="toctree-l4"><a class="reference internal" href="DecisionTree1.html">Project Induction in Decision Trees</a></li>
</ul>
</details></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../BayesEstimation/BayesEstimation.html">Bayes Estimation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l4"><a class="reference internal" href="../BayesEstimation/KDE_1.html">kernel-based density estimation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../BayesEstimation/MeanShift.html">Mean Shift Algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="../BayesEstimation/ParametricDensityEstimation1.html">Parametric Density Estimation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../BayesEstimation/GMM.html">Gaussian Mixture Model</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../machine_learning.html">Machine Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../ML/Regression/NonParamRegression.html">Regression Review</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ML/Regression/KernelRegression.html">Kernel Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ML/Regression/GP_Regression.html">Gaussian Process</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ML/FeatureReduction/FR_Intro.html">Introduction of Feature Reduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ML/FeatureReduction/PCA.html">Principal Component Analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../ML/FeatureReduction/Autoencoders1.html">Autoencoders</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../Supplementary1.html">Supplementary Documents</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../SupplementaryDocuments/EVD.html">Eigen Value Decomposition</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../SupplementaryDocuments/twin_svm.html">Twin SVM</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../SupplementaryDocuments/Covariance1.html">Covariance</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../CircuitElectronicAnalysis.html">Circuit and Electronics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../../Circuit_Electronics/Introduction_CircuitElectronics.html">Introduction to Circuit and Electronics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Circuit_Electronics/BasicElements.html">Basic component</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Circuit_Electronics/KVL_KCL.html">Voltage , Current</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Circuit_Electronics/NodalMeshAnalysis.html">Basic Nodal and Mesh Analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Circuit_Electronics/RC_RL_RLC.html">R-C, R-L circuits</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Circuit_Electronics/Diode_chapter.html">Diode</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Circuit_Electronics/DiodeApplication.html">Diode Application</a></li>


<li class="toctree-l3"><a class="reference internal" href="../../Circuit_Electronics/BJT1.html">Bipolar junction transistor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Circuit_Electronics/FET.html">Field Effect Transistor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../Circuit_Electronics/OpAmp.html">Operational Amplifier</a></li>
</ul>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../signal_processing.html">Signal Processing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../image_processing.html">Image Processing</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../PRLabProduction.html">Pattern Lab Production</a></li>



<li class="toctree-l1"><a class="reference internal" href="../../../ProposalPhD_Develop.html">Proposal PhD Develope</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../../Students.html">Students Projects</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../StudentProjects/BsC/Teymoori/chat_with_doc.html">Author Information</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../../Contact_Me.html">Contact Me</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../About_Me.html">About Me</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/h-sadoghi/dr-sadoghi.git" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/h-sadoghi/dr-sadoghi.git/issues/new?title=Issue%20on%20page%20%2Fcourses/PR/Classification/SVM1.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../../_sources/courses/PR/Classification/SVM1.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Support Vector Machine (SVM)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-support-vector-machine-svm">What is a Support Vector Machine (SVM)?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examining-some-common-concepts-in-the-support-vector-machine-algorithm">Examining Some Common Concepts in the Support Vector Machine Algorithm</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-support-vector-machine">Linear Support Vector Machine</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-relationships-and-norm-calculations">Vector Relationships and Norm Calculations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hard-margin">Hard-margin</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-insensitive-loss-function">The ε-insensitive loss function:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#characteristics-of-the-solution">Characteristics of the Solution:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#soft-margin">Soft-margin</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#non-linear-support-vector-machines">Non-linear Support Vector Machines</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-class-support-vector-classifier-svc">Multi-Class Support Vector Classifier (SVC)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-vs-rest-method">One-vs-Rest Method</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-vs-one-method">One-vs-One Method</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-and-limitations-of-svm">Challenges and Limitations of SVM</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#homework-write-svm-with-an-arbitrary-loss-function">HomeWork: Write SVM with an arbitrary loss function</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="support-vector-machine-svm">
<h1>Support Vector Machine (SVM)<a class="headerlink" href="#support-vector-machine-svm" title="Link to this heading">#</a></h1>
<p><strong>Provided By Samaneh Dandani</strong></p>
<p><strong>Contact : <a class="reference external" href="mailto:samanedandani&#37;&#52;&#48;gmail&#46;com">samanedandani<span>&#64;</span>gmail<span>&#46;</span>com</a></strong></p>
<p><img alt="Dandani" src="../../../_images/Dandani.PNG" /></p>
<p>The Support Vector Machine (SVM) algorithm was originally developed in 1964 by Vladimir Vapnik and Alexey Chervonenkis at the Institute of Control Sciences of the Russian Academy of Sciences. In 1992, Boser, Isabelle Guyon, and Vapnik introduced the kernel trick to SVM, providing a method for nonlinear classification. In 1995, Corinna Cortes and Vapnik introduced the soft-margin SVM.
The Support Vector Machine (SVM) is a supervised learning model in machine learning that, given labeled training data, constructs an optimal hyperplane to classify new data into different categories.</p>
<p><img alt="SVM0" src="../../../_images/SVM0.png" /></p>
<section id="what-is-a-support-vector-machine-svm">
<h2>What is a Support Vector Machine (SVM)?<a class="headerlink" href="#what-is-a-support-vector-machine-svm" title="Link to this heading">#</a></h2>
<p>A Support Vector Machine (SVM) is a supervised learning method used for classification and regression tasks. The basis of the SVM classifier is the linear classification of data. It separates the data using a line or hyperplane, ensuring that the separation has the maximum margin of confidence. Data on one side of the line are similar and belong to the same group. New data, when added to the same space, will be classified into one of the existing categories.
If the data cannot be separated using linear programming methods, nonlinear programming methods are used to find the optimal line. These are well-known methods for solving constrained problems. Before linear separation, to enable the machine to classify highly complex data, the data are mapped to a higher-dimensional space using a function called phi. To solve the high-dimensional problem using these methods, the Lagrangian dual theorem is employed to transform the minimization problem into its dual form. In this form, instead of the complex phi function, which maps to a high-dimensional space, a simpler function called the kernel function, which is the dot product of the phi function, appears. Various kernel functions can be used, including exponential, polynomial, and sigmoid kernels.</p>
</section>
<section id="examining-some-common-concepts-in-the-support-vector-machine-algorithm">
<h2>Examining Some Common Concepts in the Support Vector Machine Algorithm<a class="headerlink" href="#examining-some-common-concepts-in-the-support-vector-machine-algorithm" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Margin:</strong> In SVM, the margin refers to the space between the classified data groups. Think of the margin as a safe zone between two groups we want to separate; the goal is to widen this zone as much as possible so that even if new data are added, they can be classified with greater confidence. A larger margin indicates a stronger separation and, consequently, a more accurate model.</p></li>
<li><p><strong>Hyperplane:</strong>
In SVM, a hyperplane is a line or plane that separates data into two categories. In a two-dimensional space, it is a straight line; however, in higher-dimensional spaces, it becomes a larger plane that can separate data across different dimensions. The optimal hyperplane is the one that creates the largest margin between the different classes of data.
Feature Space
The feature space refers to the dimension or domain in which the data are represented and analyzed. In SVM, transforming data into a higher-dimensional feature space allows for the linear separation of data that are not linearly separable in their original dimensions.</p></li>
<li><p><strong>Kernels:</strong>
Kernels in SVM allow us to transform data that are not linearly separable into a higher-dimensional feature space where separation is possible. There are various types of kernels, each suitable for different data and problems. The most common kernels include the linear kernel, polynomial kernel, radial basis function (RBF) kernel, and sigmoid kernel.</p></li>
</ul>
<section id="linear-support-vector-machine">
<h3>Linear Support Vector Machine<a class="headerlink" href="#linear-support-vector-machine" title="Link to this heading">#</a></h3>
<p>Given a dataset consisting of <span class="math notranslate nohighlight">\( n \)</span> elements, defined as follows:</p>
<div class="math notranslate nohighlight">
\[
D = \{(x_i, y_i) \mid x_i \in \mathbb{R}^P, y_i \in \{-1, 1\}\}_{i=1}^n
\]</div>
<p>The value of <span class="math notranslate nohighlight">\( y \)</span> is either 1 or -1, and each <span class="math notranslate nohighlight">\( x_i \)</span> is a real-valued vector of <span class="math notranslate nohighlight">\( P \)</span> dimensions. The goal is to find the separating hyperplane with the maximum margin from the support vectors that separate the points with <span class="math notranslate nohighlight">\( y_i = 1 \)</span> from the points with <span class="math notranslate nohighlight">\( y_i = -1 \)</span>. Each hyperplane can be written as a set of points that satisfy the following condition:</p>
<div class="math notranslate nohighlight">
\[
w^T \cdot x + b = 0
\]</div>
<p>where <span class="math notranslate nohighlight">\( \cdot \)</span> denotes the dot product, <span class="math notranslate nohighlight">\( w \)</span> is the normal vector, which is perpendicular to the hyperplane, and <span class="math notranslate nohighlight">\( b \)</span> is the bias term. We want to choose <span class="math notranslate nohighlight">\( w \)</span> and <span class="math notranslate nohighlight">\( b \)</span> such that the distance between the parallel hyperplanes that separate the data is maximized. These hyperplanes are described by the following relation:</p>
<p>Any data point above the separating hyperplane is labeled as 1:</p>
<div class="math notranslate nohighlight">
\[
w^T \cdot x + b = 1
\]</div>
<p>And any data point below the separating hyperplane is labeled as -1:</p>
<div class="math notranslate nohighlight">
\[
w^T \cdot x + b = -1
\]</div>
<p><img alt="SVM_1" src="../../../_images/SVM1.png" /></p>
<p>Now we want to determine the margin m:</p>
<p><img alt="SVM_2" src="../../../_images/SVM2.png" /></p>
<p>We focus on the right-angled triangle with vertices 〖〖P,x〗^((1)),x〗^((0)):</p>
<p><img alt="SVM_3" src="../../../_images/SVM3.png" /></p>
<p>For simplicity, we name the vectors as follows:</p>
<ol class="arabic simple">
<li><p><strong>Vector Addition</strong>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\vec{B} = \vec{A} + \vec{C}
\]</div>
<p>This states that vector <span class="math notranslate nohighlight">\(\vec{B}\)</span> is the sum of vectors <span class="math notranslate nohighlight">\(\vec{A}\)</span> and <span class="math notranslate nohighlight">\(\vec{C}\)</span>.</p>
<ol class="arabic simple" start="2">
<li><p><strong>Difference of Two Points</strong>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\vec{B} = x^{(1)} - x^{(0)}
\]</div>
<p>Here, <span class="math notranslate nohighlight">\(\vec{B}\)</span> represents the vector obtained by subtracting point <span class="math notranslate nohighlight">\(x^{(0)}\)</span> from point <span class="math notranslate nohighlight">\(x^{(1)}\)</span>.</p>
<ol class="arabic simple" start="3">
<li><p><strong>Scaled Normal Vector</strong>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\vec{A} = \frac{\|\vec{A}\| \vec{w}}{\|\vec{w}\|}
\]</div>
<p>Vector <span class="math notranslate nohighlight">\(\vec{A}\)</span> is the normal vector <span class="math notranslate nohighlight">\(\vec{w}\)</span> scaled by the ratio of the magnitudes of <span class="math notranslate nohighlight">\(\vec{A}\)</span> and <span class="math notranslate nohighlight">\(\vec{w}\)</span>.</p>
<ol class="arabic simple" start="4">
<li><p><strong>Orthogonality Condition</strong>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\vec{A}^T \vec{C} = 0
\]</div>
<p>This indicates that vectors <span class="math notranslate nohighlight">\(\vec{A}\)</span> and <span class="math notranslate nohighlight">\(\vec{C}\)</span> are orthogonal (their dot product is zero).</p>
</section>
<section id="vector-relationships-and-norm-calculations">
<h3>Vector Relationships and Norm Calculations<a class="headerlink" href="#vector-relationships-and-norm-calculations" title="Link to this heading">#</a></h3>
<p>Given the vector relationships and the goal to derive the margin in a Support Vector Machine context, let’s proceed with the step-by-step derivation.</p>
<ol class="arabic simple">
<li><p><strong>Starting Equation</strong>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\vec{A}^T \vec{B} = \vec{A}^T (\vec{A} + \vec{C})
\]</div>
<ol class="arabic simple" start="2">
<li><p><strong>Expanding the Dot Product</strong>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\vec{A}^T \vec{B} = \vec{A}^T \vec{A} + \vec{A}^T \vec{C}
\]</div>
<ol class="arabic simple" start="3">
<li><p><strong>Orthogonality Condition</strong>:
Since <span class="math notranslate nohighlight">\(\vec{A}\)</span> and <span class="math notranslate nohighlight">\(\vec{C}\)</span> are orthogonal:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\vec{A}^T \vec{C} = 0
\]</div>
<ol class="arabic simple" start="4">
<li><p><strong>Norm of Vector <span class="math notranslate nohighlight">\(\vec{A}\)</span></strong>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\vec{A}^T \vec{A} = \|\vec{A}\|^2
\]</div>
<p>So,</p>
<div class="math notranslate nohighlight">
\[
\vec{A}^T \vec{B} = \|\vec{A}\|^2
\]</div>
<ol class="arabic simple" start="5">
<li><p><strong>Substituting <span class="math notranslate nohighlight">\(\vec{A}\)</span></strong>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\|\vec{A}\|^2 = \vec{A}^T \vec{B} = \left(\frac{\|\vec{A}\| \vec{w}}{\|\vec{w}\|}\right)^T \vec{B}
\]</div>
<ol class="arabic simple" start="6">
<li><p><strong>Expanding <span class="math notranslate nohighlight">\(\vec{B}\)</span></strong>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\vec{B} = x^{(1)} - x^{(0)}
\]</div>
<ol class="arabic simple" start="7">
<li><p><strong>Dot Product with <span class="math notranslate nohighlight">\(\vec{w}\)</span></strong>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\|\vec{A}\|^2 = \frac{\|\vec{A}\|}{\|\vec{w}\|} \vec{w}^T (x^{(1)} - x^{(0)})
\]</div>
<ol class="arabic simple" start="8">
<li><p><strong>Solving for <span class="math notranslate nohighlight">\(\|\vec{A}\|\)</span></strong>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\|\vec{A}\| = \frac{1}{\|\vec{w}\|} \vec{w}^T (x^{(1)} - x^{(0)})
\]</div>
<ol class="arabic simple" start="9">
<li><p><strong>Using Hyperplane Definitions</strong>:
For <span class="math notranslate nohighlight">\( x^{(1)} \)</span> on <span class="math notranslate nohighlight">\( w^T x + b = 1 \)</span> and <span class="math notranslate nohighlight">\( x^{(0)} \)</span> on <span class="math notranslate nohighlight">\( w^T x + b = -1 \)</span>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\vec{w}^T x^{(1)} + b = 1\]</div>
<div class="math notranslate nohighlight">
\[
\vec{w}^T x^{(0)} + b = -1
\]</div>
<ol class="arabic simple" start="10">
<li><p><strong>Subtracting the Hyperplane Equations</strong>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\vec{w}^T (x^{(1)} - x^{(0)}) = 2
\]</div>
<ol class="arabic simple" start="11">
<li><p><strong>Final Norm Calculation</strong>:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\|\vec{A}\| = \frac{1}{\|\vec{w}\|} \cdot 2 = \frac{2}{\|\vec{w}\|}
\]</div>
<p>Therefore, the margin <span class="math notranslate nohighlight">\( m \)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[
m = \|\vec{A}\| = \frac{2}{\|\vec{w}\|}
\]</div>
<p>In Support Vector Machines, we aim to maximize the margin, which is the distance between the supporting vectors. Therefore, we minimize <span class="math notranslate nohighlight">\(\|\vec{w}\|\)</span></p>
<p>To maximize the margin <span class="math notranslate nohighlight">\( m \)</span> in a Support Vector Machine (SVM), you aim to maximize the distance between the separating hyperplanes. Given the relationship between the margin and the norm of the normal vector <span class="math notranslate nohighlight">\( \vec{w} \)</span>, you can express this as follows:</p>
<div class="math notranslate nohighlight">
\[
m = \|\vec{A}\| = \frac{2}{\|\vec{w}\|}
\]</div>
<p>The goal is to maximize <span class="math notranslate nohighlight">\( m \)</span>, which translates to:</p>
<div class="math notranslate nohighlight">
\[
\max m = \max \|\vec{A}\| = \frac{2}{\min \|\vec{w}\|}
\]</div>
</section>
</section>
<section id="hard-margin">
<h2>Hard-margin<a class="headerlink" href="#hard-margin" title="Link to this heading">#</a></h2>
<p>For the hard-margin case, in the simple scenario where the training data is linearly separable, the goal is to find a separating hyperplane that maximizes the margin between the two classes of data points. We can consider two parallel hyperplanes that do not intersect any data points and then aim to maximize the distance between them. The distance between these two hyperplanes is <span class="math notranslate nohighlight">\( \frac{2}{\|\vec{w}\|} \)</span>
Therefore, we need to minimize <span class="math notranslate nohighlight">\(\|\vec{w}\|\)</span></p>
<p>To prevent data points from entering the margin, we introduce the following conditions for each i:</p>
<div class="math notranslate nohighlight">
\[ y_i (w^T x + b) \geq 1 \quad \forall i = 1, \ldots, n \]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{cases}
y_i (w^T x_i + b) \geq 1 &amp; y_i = 1 \\
y_i (w^T x_i + b) \leq -1 &amp; y_i = -1
\end{cases}
\end{split}\]</div>
<p><img alt="SVM_4" src="../../../_images/SVM4.png" /></p>
<section id="the-insensitive-loss-function">
<h3>The ε-insensitive loss function:<a class="headerlink" href="#the-insensitive-loss-function" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[
L(e) = \sum_{i=1}^n \max(|e_i| - \epsilon, 0)
\]</div>
<p>We write the expression as follows:</p>
<div class="math notranslate nohighlight">
\[
\min \sum_{i=1}^n L(e_i) + \lambda \|\vec{w}\|^2
\]</div>
<div class="math notranslate nohighlight">
\[
= \min \sum_{i=1}^n \max(|e_i| - \epsilon, 0) + \lambda \|\vec{w}\|^2
\]</div>
<div class="math notranslate nohighlight">
\[
= \min \sum_{i=1}^n \xi_i + \lambda \|\vec{w}\|^2
\]</div>
<p>subject to:</p>
<div class="math notranslate nohighlight">
\[
\xi_i \geq 0 \quad \text{for } i = 1, \ldots, n
\]</div>
<div class="math notranslate nohighlight">
\[
\xi_i \geq 1 - y_i (\vec{w}^T x_i + b) \quad \text{for } i = 1, \ldots, n
\]</div>
<p>In which <span class="math notranslate nohighlight">\( \|\vec{w}\| = \sum_{j=1}^d |w_j| \)</span> represents the 1-norm. This optimization problem is challenging because it depends on <span class="math notranslate nohighlight">\( \|\vec{w}\| \)</span> (the norm or length of the vector). Now, without changing the problem, we replace <span class="math notranslate nohighlight">\( \|\vec{w}\|^2 \)</span> with <span class="math notranslate nohighlight">\( \frac{1}{2} \|\vec{w}\|^2 \)</span> (the factor of <span class="math notranslate nohighlight">\( \frac{1}{2} \)</span> is for mathematical convenience). This is a quadratic programming (QP) non-linear optimization (OP) problem.</p>
<p>You can express the previous phrase using Lagrange as follows:</p>
<div class="math notranslate nohighlight">
\[
\min_{(w, b)} Q(w, b, \alpha) = \min \frac{1}{2} \|\vec{w}\|^2 + \sum_{i=1}^n \alpha_i \left(1 - y_i (\vec{w}^T x_i + b)\right)
\]</div>
<p>Now we take the derivative with respect to <span class="math notranslate nohighlight">\( \vec{w} \)</span> and <span class="math notranslate nohighlight">\( b \)</span>:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial Q}{\partial \vec{w}} = 0 \quad \Rightarrow \quad \vec{w} = \sum_{i=1}^n \alpha_i y_i x_i
\]</div>
<div class="math notranslate nohighlight">
\[
\frac{\partial Q}{\partial b} = 0 \quad \Rightarrow \quad \sum_{i=1}^n \alpha_i y_i = 0
\]</div>
<div class="math notranslate nohighlight">
\[
Q = \frac{1}{2} \sum_{i=1}^n \alpha_i y_i x_i^T \sum_{i=1}^n \alpha_i y_i x_i + \sum_{i=1}^n \alpha_i \left(1 - y_i \left(\sum_{j=1}^\Pi \alpha_j y_j x_j^T x_i + b\right)\right)
\]</div>
<div class="math notranslate nohighlight">
\[
= \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_j + \sum_{j=1}^n \alpha_i - \sum_{j=1}^n \alpha_i y_i \sum_{j=1}^n \alpha_j y_j x_j^T x_i - b \sum_{j=1}^n \alpha_i y_i
\]</div>
<div class="math notranslate nohighlight">
\[
= -\frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_j + \sum_{j=1}^n \alpha_i
\]</div>
<p>We note that <span class="math notranslate nohighlight">\( \alpha_i \geq 0 \)</span> and <span class="math notranslate nohighlight">\( \sum_{i=1}^n \alpha_i y_i = 0 \)</span>. Now, we have managed to express the Lagrange function in terms of a single variable, <span class="math notranslate nohighlight">\( \alpha \)</span>. In mathematics, this problem is recognized as the dual problem. If we find <span class="math notranslate nohighlight">\( \vec{w} \)</span>, then we will be able to determine all <span class="math notranslate nohighlight">\( \alpha_i \)</span>, and vice versa. The derived function from the dual problem must be maximized. Therefore, the problem is formulated as follows:</p>
<div class="math notranslate nohighlight">
\[
\max Q(\alpha) = \sum_{j=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_j
\]</div>
<p>subject to:
$<span class="math notranslate nohighlight">\(
\sum_{i=1}^n \alpha_i y_i = 0
\)</span>$</p>
<div class="math notranslate nohighlight">
\[
\alpha_i \geq 0
\]</div>
<p>This is a nonlinear programming problem where <span class="math notranslate nohighlight">\( \vec{w} \)</span> is obtained using the formula <span class="math notranslate nohighlight">\( \vec{w} = \sum_{i=1}^n \alpha_i y_i x_i \)</span>, and once <span class="math notranslate nohighlight">\( \vec{w} \)</span> is determined, each <span class="math notranslate nohighlight">\( \alpha_i \)</span> can be computed.</p>
</section>
<section id="characteristics-of-the-solution">
<h3>Characteristics of the Solution:<a class="headerlink" href="#characteristics-of-the-solution" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p>Most of the <span class="math notranslate nohighlight">\( \alpha_i \)</span>‘s obtained are zero.</p></li>
<li><p>The <span class="math notranslate nohighlight">\( x_i \)</span>‘s corresponding to non-zero <span class="math notranslate nohighlight">\( \alpha_i \)</span>‘s are recognized as Support Vectors (SV).</p></li>
<li><p><span class="math notranslate nohighlight">\( \vec{w} \)</span> is a linear combination of a small number of data points.</p></li>
</ol>
<p>Using the Support Vectors (SV) and the obtained <span class="math notranslate nohighlight">\( \vec{w} \)</span>, <span class="math notranslate nohighlight">\( b \)</span> can also be calculated:</p>
<div class="math notranslate nohighlight">
\[
b = y_i - \vec{w}^T x_i
\]</div>
<p>Classification can also be derived from the following relationship:</p>
<div class="math notranslate nohighlight">
\[
D(x) = \text{sign}(\vec{w}^T x + b) = \text{sign}\left(\sum_{i=1}^n \alpha_i y_i x_i^T x + b\right)
\]</div>
</section>
</section>
<section id="soft-margin">
<h2>Soft-margin<a class="headerlink" href="#soft-margin" title="Link to this heading">#</a></h2>
<p>Using the hard-margin method can impose constraints in certain cases, one of which is a strong dependence of the separating hyperplane on the boundary data points. If a data point lies on the boundary of the separating hyperplane, it can lead to a reduction in margin and significant changes in the intended hyperplane. This point is crucial because real-world data often contain some level of noise, potentially resulting in undesirable effects on the decision boundary. Therefore, hard-margin models are highly susceptible to overfitting.</p>
<p><img alt="SVM_5" src="../../../_images/SVM5.png" /></p>
<p>To prevent and address this issue, the concept of soft-margin was introduced, allowing some data points to be misclassified during the model training process and violating the determined margin. This approach aims to prevent overfitting during testing. In this scenario, we tolerate classification errors. Sometimes, a few data points from one class, especially when their number is not very large, are situated among data points of another class. In such cases, it is not straightforward to find an optimal separating hyperplane that completely separates the two classes. In other words, this method introduces a level of generalization to the model to perform better on test data.</p>
<p>It’s important to note that to prevent a decrease in model accuracy, this approach is controlled by adding a series of parameters known as slack variables, denoted by <span class="math notranslate nohighlight">\( \xi \)</span>. These variables quantify the extent to which each data point deviates from the prescribed margin. If their value is zero, it indicates that the data point is correctly positioned with respect to the margin. If their value is greater than zero, it signifies the degree of error.</p>
<p>Therefore, to incorporate the concept of proximity of data points to other classes, we use the hinge loss function:</p>
<div class="math notranslate nohighlight">
\[
L(e) = \sum_{i=1}^n \max(\rho - y_i \hat{y}_i, 0)
\]</div>
<p>We also write the expression like this:</p>
<div class="math notranslate nohighlight">
\[
\min \sum_{i=1}^n L(e_i) + \lambda \|\vec{w}\|^2
\]</div>
<div class="math notranslate nohighlight">
\[
= \min \sum_{i=1}^n \max(\rho - y_i \hat{y}_i, 0) + \lambda \|\vec{w}\|^2
\]</div>
<div class="math notranslate nohighlight">
\[
= \min \sum_{i=1}^n \xi_i + \lambda \|\vec{w}\|^2
\]</div>
<p>subject to</p>
<div class="math notranslate nohighlight">
\[
\xi_i \geq 0 \quad \text{for} \quad i = 1, \ldots, n
\]</div>
<div class="math notranslate nohighlight">
\[
\xi_i \geq \rho - y_i (\vec{w}^T \vec{x}_i + b) \quad \text{for} \quad i = 1, \ldots, n
\]</div>
<p>Here we take <span class="math notranslate nohighlight">\( \rho = 1 \)</span> and set <span class="math notranslate nohighlight">\( \lambda = C \)</span> and <span class="math notranslate nohighlight">\( C \)</span> behind <span class="math notranslate nohighlight">\( \sum \)</span>:</p>
<div class="math notranslate nohighlight">
\[
= \min \frac{1}{2} \|\vec{w}\|^2 + C \sum_{i=1}^n \xi_i
\]</div>
<p>subject to</p>
<div class="math notranslate nohighlight">
\[
\xi_i \geq 0 \quad \text{for} \quad i = 1, \ldots, n
\]</div>
<div class="math notranslate nohighlight">
\[
y_i (\vec{w}^T \vec{x}_i + b) \geq 1 - \xi_i \quad \text{for} \quad i = 1, \ldots, n
\]</div>
<p>In fact, we allow for classification errors. Sometimes, some data points from one class, which are not very numerous, are placed inside the data points of another class, making it difficult to find an optimal separating hyperplane that completely separates the two classes. In this case, using the auxiliary variable <span class="math notranslate nohighlight">\( \xi \)</span>, which corresponds to each data point, meaning classification error, is permitted. Essentially, <span class="math notranslate nohighlight">\( \xi_i \)</span> is an auxiliary variable in optimization. It is evident that if <span class="math notranslate nohighlight">\( \xi_i = 0 \)</span>, it means the data point <span class="math notranslate nohighlight">\( \vec{x}_i \)</span> is correctly classified and belongs to its own class. If their values are greater than zero, it indicates they represent classification errors.</p>
<div class="math notranslate nohighlight">
\[
\xi = (\xi_1, \xi_2, \ldots, \xi_n)
\]</div>
<p><img alt="SVM_6" src="../../../_images/SVM6.png" /></p>
<p>Taking into account the error, we have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{cases}
y_i (w^T x_i + b) \geq 1 - \xi_i &amp; \text{if } y_i = 1 \\
y_i (w^T x_i + b) \leq -1 + \xi_i &amp; \text{if } y_i = -1 \\
\xi_i \geq 0 &amp;
\end{cases}
\end{split}\]</div>
<p>The purpose of including the term <span class="math notranslate nohighlight">\( C\sum_{i=1}^n \xi_i \)</span> in the equation is to control the number of misclassified points, known as the penalty coefficient. Essentially, <span class="math notranslate nohighlight">\( C \)</span> is a regularization parameter that is initially set. Adjusting this parameter can strike a balance between maximizing the margin between classes and controlling classification errors. A larger <span class="math notranslate nohighlight">\( C \)</span> prevents <span class="math notranslate nohighlight">\( \xi_i \)</span> from becoming large, thereby restricting the margin between classes. Choosing a smaller <span class="math notranslate nohighlight">\( C \)</span> allows for more classification errors. In this scenario, more importance is given to widening the margin between classes. This problem is also solved similarly to the hard-margin case. The Lagrange function in this case is as follows:</p>
<div class="math notranslate nohighlight">
\[
Q(w,b,\xi,\alpha) = \frac{1}{2} \|w\|^2 + C\sum_{i=1}^n \xi_i - \sum_{i=1}^n \alpha_i \{y_i (w^T x_i + b) - 1 + \xi_i \} - \sum_{i=1}^n \beta_i \xi_j
\]</div>
<p>The coefficients <span class="math notranslate nohighlight">\( \alpha \)</span> and <span class="math notranslate nohighlight">\( \beta \)</span> are Lagrange multipliers. To obtain the optimal separating hyperplane, the Karush-Kuhn-Tucker (KKT) conditions must be satisfied:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\frac{\partial Q(w,b,\xi,\alpha)}{\partial w} = 0 &amp; \Rightarrow w = \sum_{i=1}^n \alpha_i y_i x_i \\
\frac{\partial Q(w,b,\xi,\alpha)}{\partial b} = 0 &amp; \Rightarrow \sum_{i=1}^n \alpha_i y_i = 0 \\
\frac{\partial Q(w,b,\xi,\beta,\gamma)}{\partial \xi} = 0 &amp; \Rightarrow \alpha_i + \beta_i = C
\end{align*}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{cases}
\alpha_i \{y_i (w^T x_i + b) - 1 + \xi_i \} = 0 \\
\beta_i \xi_i = 0 \\
\xi_i \geq 0 \\
\alpha_i \geq 0 \\
\beta_i \geq 0
\end{cases}
\end{split}\]</div>
<p>The optimization problem will be as follows:</p>
<div class="math notranslate nohighlight">
\[
\max Q(\alpha) = \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j x_i^T x_j
\]</div>
<p>subject to</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{cases}
\sum_{i=1}^n \alpha_i y_i = 0 \\
0 \leq \alpha_i \leq C \quad i = 1, \ldots, n
\end{cases}
\end{split}\]</div>
<p>And the decision function is as follows:</p>
<div class="math notranslate nohighlight">
\[
f(x) = \text{sign}(w^T x + b) = \text{sign} \left( \sum_{i \in S} \alpha_i y_i x_i^T x + b \right)
\]</div>
<p><span class="math notranslate nohighlight">\( S \)</span> is the set of support vectors.</p>
<section id="non-linear-support-vector-machines">
<h3>Non-linear Support Vector Machines<a class="headerlink" href="#non-linear-support-vector-machines" title="Link to this heading">#</a></h3>
<p>In 1992, Bernhard Boser, Isabelle Guyon, and Vladimir Vapnik proposed a method for creating non-linear classifiers by introducing a kernel to find the hyperplane with maximum margin. The proposed algorithm appears similar, except that all dot products are replaced with a non-linear kernel function. This property allows the algorithm to be suitable for finding the hyperplane with maximum margin in a transformed feature space. However, the classifier is a hyperplane in a high-dimensional feature space, which corresponds to a non-linear space in the input space.</p>
<p>The points <span class="math notranslate nohighlight">\( x_i \)</span> are transformed into a high-dimensional space, where a linear decision surface is found. Thus, we provide the following definitions:</p>
<ul class="simple">
<li><p>Input Space: The space where the points <span class="math notranslate nohighlight">\( x_i \)</span> reside.</p></li>
<li><p>Feature Space: The space created after applying a function like <span class="math notranslate nohighlight">\( \phi(x_i) \)</span> to <span class="math notranslate nohighlight">\( x_i \)</span>, typically in high dimensions.</p></li>
</ul>
<p><img alt="SVM_7" src="../../../_images/SVM7.png" /></p>
<p>Given the linear operations in the <span class="math notranslate nohighlight">\( \phi \)</span> space are equivalent to nonlinear operations in the input space, the transformation by <span class="math notranslate nohighlight">\( \phi \)</span> makes the type of operations easier. It should be noted that the <span class="math notranslate nohighlight">\( \phi \)</span> space effectively has higher dimensions compared to the input space. In practice, it may have infinite dimensions. To address this issue and avoid the mentioned problem, a method called the Kernel Trick is used. Consider the following kernel function:</p>
<div class="math notranslate nohighlight">
\[
k(x_i, x_j) = \phi^T (x_i) \cdot \phi(x_j)
\]</div>
<p>Then the dual problem in the <span class="math notranslate nohighlight">\( \phi \)</span> space will be as follows:</p>
<div class="math notranslate nohighlight">
\[
\text{Max} \quad Q(\alpha) = \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j \phi(x_i^T) \phi(x_j)
\]</div>
<p>subject to</p>
<div class="math notranslate nohighlight">
\[
\sum_{i=1}^n \alpha_i y_i = 0
\]</div>
<div class="math notranslate nohighlight">
\[ 0 \leq \alpha_i \leq C \quad \text{for} \quad i=1,...,n \]</div>
</section>
</section>
<section id="multi-class-support-vector-classifier-svc">
<h2>Multi-Class Support Vector Classifier (SVC)<a class="headerlink" href="#multi-class-support-vector-classifier-svc" title="Link to this heading">#</a></h2>
<p>Despite its numerous advantages, SVM has limitations. One of these limitations is that it was originally designed for separating two classes, and extending it to handle multi-class separation poses challenges with no straightforward unique method available. Multi-class classification can be achieved by combining binary SVM classifiers.
Typically, there are two main approaches for this purpose. One is the “one-vs-rest” strategy for classifying each class against all others. The other is the “one-vs-one” strategy for classifying each pair of classes. In some cases, the “one-vs-rest” strategy can lead to ambiguous classification. For multi-class problems, a common approach is to reduce the problem into several binary classification problems. Each of these problems is solved with a binary separator SVM. Subsequently, the outputs of these binary separators are combined to solve the multi-class problem.</p>
<section id="one-vs-rest-method">
<h3>One-vs-Rest Method<a class="headerlink" href="#one-vs-rest-method" title="Link to this heading">#</a></h3>
<p>In this method, we need hyperplanes that separate data of one class from data of all other classes. Essentially, all samples are considered in obtaining the optimal hyperplane, such that for each classifier, data are divided into two groups: one group for the data of the target class and another group consisting of all data from the other classes. Therefore, for a classification problem with K classes, we require K binary classifiers. During training, all K classifiers are trained on the training set, and then during testing, we check which classifier classifies the input data into its class. In some cases, multiple classifiers may classify the data into their respective classes simultaneously. In such cases, metrics can be used, and one of these metrics is selecting the classifier that has the farthest hyperplane from the target data.</p>
<p><img alt="SVM_8" src="../../../_images/SVM8.png" /></p>
</section>
<section id="one-vs-one-method">
<h3>One-vs-One Method<a class="headerlink" href="#one-vs-one-method" title="Link to this heading">#</a></h3>
<p>In this method, unlike the previous approach, we aim to find a classifier between every pair of classes. Therefore, we need <span class="math notranslate nohighlight">\(\binom{n}{2}\)</span> SVM classifiers, where <span class="math notranslate nohighlight">\(n\)</span> is the number of classes. This is significantly more compared to the previous method. Thus, the problem of <span class="math notranslate nohighlight">\(n\)</span> classes is transformed into <span class="math notranslate nohighlight">\( \frac{n(n - 1)}{2} \)</span> binary classification problems. However, each SVM in training uses only the data from those two classes, requiring less time compared to the previous method.</p>
<p>Here, we define the separating hyperplanes as follows, where the hyperplane separates class <span class="math notranslate nohighlight">\(i\)</span> from class <span class="math notranslate nohighlight">\(j\)</span>:</p>
<div class="math notranslate nohighlight">
\[ D_{ij} = W_{ij}^T \phi(x) + b_{ij} \]</div>
<p>Now, for the test data <span class="math notranslate nohighlight">\(x\)</span>, we proceed by defining a region <span class="math notranslate nohighlight">\(R_i\)</span> for each class as follows:</p>
<div class="math notranslate nohighlight">
\[ R_i = \{ x \mid D_{ij}(x) &gt; 0, \; j = 1, 2, \ldots, n, \; j \ne i \} \]</div>
<p>In any region where this condition holds, meaning all <span class="math notranslate nohighlight">\(D_{ij}\)</span> are positive, it belongs to the class corresponding to that region. Such a region may not always be found; in that case, <span class="math notranslate nohighlight">\(x\)</span> belongs to class <span class="math notranslate nohighlight">\(D_i(x)\)</span>.</p>
<div class="math notranslate nohighlight">
\[ \arg \max_{i=1,2,\ldots,n} D_i(x) \]</div>
<div class="math notranslate nohighlight">
\[ D_i(x) = \sum_{i \ne j, j=1}^n \operatorname{sign}(D_{ij}(x)) \]</div>
<p><img alt="SVM_9" src="../../../_images/SVM9.png" /></p>
</section>
<section id="challenges-and-limitations-of-svm">
<h3>Challenges and Limitations of SVM<a class="headerlink" href="#challenges-and-limitations-of-svm" title="Link to this heading">#</a></h3>
<p>Using Support Vector Machine (SVM) in machine learning offers many advantages, but like any other method, it has its own specific challenges and limitations. Here, we discuss some of these constraints and challenges:</p>
<p><strong>Kernel Selection</strong>
One of the major challenges in using SVM is selecting the appropriate kernel type. If the chosen kernel does not align well with the nature of the data, the final model may fail to learn complex patterns, leading to decreased prediction accuracy.</p>
<p><strong>Feature Scaling</strong>
The SVM algorithm is sensitive to feature scaling. Features that are on larger scales can disproportionately influence the final model, causing the decision boundary to lean towards features with larger scales. Therefore, feature scaling prior to training the model is crucial.</p>
<p><strong>Imbalanced Data</strong>
When training data is imbalanced, meaning one class has significantly more samples than others, SVM may become biased towards the class with more samples. This issue can result in incorrect classification of classes with fewer samples.</p>
<p><strong>Heavy Computations</strong>
Although SVM performs well on datasets with moderate dimensions, the training time of the model significantly increases with the increase in the number of features and samples. This computational challenge makes SVM less efficient for large and complex datasets.</p>
<p><strong>Parameter Tuning</strong>
Determining suitable parameters for SVM can be challenging. Parameters such as C (error cost) and kernel parameters need to be carefully tuned to prevent overfitting or underfitting of the model.</p>
<p><strong>Interpretability</strong>
While SVM can accurately detect complex patterns, interpreting the models produced by it can be difficult. This issue is more pronounced when complex kernels are used, making model interpretation challenging.
These challenges and limitations highlight the importance of understanding the characteristics of SVM and carefully considering its application in different scenarios.</p>
</section>
</section>
<section id="homework-write-svm-with-an-arbitrary-loss-function">
<h2>HomeWork: Write SVM with an arbitrary loss function<a class="headerlink" href="#homework-write-svm-with-an-arbitrary-loss-function" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Mathematics</p></li>
<li><p>Code (without using pre-written functions)
(see following one)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">minimize</span>

<span class="c1"># Define the RBF kernel function</span>
<span class="k">def</span> <span class="nf">rbf_kernel</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x1</span> <span class="o">-</span> <span class="n">x2</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># Compute the kernel matrix for a dataset</span>
<span class="k">def</span> <span class="nf">compute_kernel_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
    <span class="n">n_samples</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">):</span>
            <span class="n">K</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">rbf_kernel</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">j</span><span class="p">],</span> <span class="n">gamma</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">K</span>

<span class="c1"># Objective function to minimize</span>
<span class="k">def</span> <span class="nf">objective_function</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">C</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">alpha</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">alpha</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">y</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">y</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">K</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span>

<span class="c1"># Constraint functions</span>
<span class="k">def</span> <span class="nf">constraint_eq</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Training function</span>
<span class="k">def</span> <span class="nf">train_svm</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
    <span class="n">n_samples</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">K</span> <span class="o">=</span> <span class="n">compute_kernel_matrix</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
    <span class="n">alpha_init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
    
    <span class="c1"># Define constraints and bounds</span>
    <span class="n">constraints</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;eq&#39;</span><span class="p">,</span> <span class="s1">&#39;fun&#39;</span><span class="p">:</span> <span class="n">constraint_eq</span><span class="p">,</span> <span class="s1">&#39;args&#39;</span><span class="p">:</span> <span class="p">(</span><span class="n">y</span><span class="p">,)}</span>
    <span class="n">bounds</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)]</span>
    
    <span class="c1"># Minimize the objective function</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">objective_function</span><span class="p">,</span> <span class="n">alpha_init</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">C</span><span class="p">),</span> <span class="n">bounds</span><span class="o">=</span><span class="n">bounds</span><span class="p">,</span> <span class="n">constraints</span><span class="o">=</span><span class="n">constraints</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">result</span><span class="o">.</span><span class="n">x</span>

<span class="c1"># Prediction function</span>
<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
    <span class="n">K_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">rbf_kernel</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span> <span class="k">for</span> <span class="n">x_train</span> <span class="ow">in</span> <span class="n">X_train</span><span class="p">]</span> <span class="k">for</span> <span class="n">x_test</span> <span class="ow">in</span> <span class="n">X_test</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">K_test</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">y</span><span class="p">))</span>

<span class="c1"># Visualization function</span>
<span class="k">def</span> <span class="nf">plot_decision_boundary</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
    <span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
    <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">),</span>
                         <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">))</span>
    
    <span class="n">Z</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()],</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
    <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    
    <span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">bwr</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">bwr</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;SVM Decision Boundary with RBF Kernel&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Feature 1&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Feature 2&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Generate synthetic dataset</span>
<span class="k">def</span> <span class="nf">generate_synthetic_data</span><span class="p">(</span><span class="n">n_samples</span><span class="p">):</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">((</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span>

<span class="c1"># Example usage</span>
<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="c1"># Generate synthetic data</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">generate_synthetic_data</span><span class="p">(</span><span class="mi">30</span><span class="p">)</span>
    
    <span class="c1"># Hyperparameters</span>
    <span class="n">C</span> <span class="o">=</span> <span class="mf">1.0</span>
    <span class="n">gamma</span> <span class="o">=</span> <span class="mf">1.0</span>
    
    <span class="c1"># Train SVM</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">train_svm</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
    
    <span class="c1"># Plot decision boundary</span>
    <span class="n">plot_decision_boundary</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">KeyboardInterrupt</span><span class="g g-Whitespace">                         </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">line</span> <span class="mi">83</span>
<span class="g g-Whitespace">     </span><span class="mi">80</span> <span class="n">alpha</span> <span class="o">=</span> <span class="n">train_svm</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">82</span> <span class="c1"># Plot decision boundary</span>
<span class="ne">---&gt; </span><span class="mi">83</span> <span class="n">plot_decision_boundary</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>

<span class="nn">Cell In[1], line 53,</span> in <span class="ni">plot_decision_boundary</span><span class="nt">(X, y, alpha, gamma)</span>
<span class="g g-Whitespace">     </span><span class="mi">49</span> <span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="g g-Whitespace">     </span><span class="mi">50</span> <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">),</span>
<span class="g g-Whitespace">     </span><span class="mi">51</span>                      <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">))</span>
<span class="ne">---&gt; </span><span class="mi">53</span> <span class="n">Z</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()],</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">54</span> <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">56</span> <span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">bwr</span><span class="p">)</span>

<span class="nn">Cell In[1], line 43,</span> in <span class="ni">predict</span><span class="nt">(X_train, X_test, alpha, y, gamma)</span>
<span class="g g-Whitespace">     </span><span class="mi">42</span> <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
<span class="ne">---&gt; </span><span class="mi">43</span>     <span class="n">K_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="n">rbf_kernel</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">gamma</span><span class="p">)</span> <span class="k">for</span> <span class="n">x_train</span> <span class="ow">in</span> <span class="n">X_train</span><span class="p">]</span> <span class="k">for</span> <span class="n">x_test</span> <span class="ow">in</span> <span class="n">X_test</span><span class="p">])</span>
<span class="g g-Whitespace">     </span><span class="mi">44</span>     <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">K_test</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">y</span><span class="p">))</span>

<span class="nn">Cell In[1], line 7,</span> in <span class="ni">rbf_kernel</span><span class="nt">(x1, x2, gamma)</span>
<span class="g g-Whitespace">      </span><span class="mi">6</span> <span class="k">def</span> <span class="nf">rbf_kernel</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
<span class="ne">----&gt; </span><span class="mi">7</span>     <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x1</span> <span class="o">-</span> <span class="n">x2</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="nn">File E:\MainHomePage\.M_HomePage\Lib\site-packages\numpy\linalg\_linalg.py:2736,</span> in <span class="ni">norm</span><span class="nt">(x, ord, axis, keepdims)</span>
<span class="g g-Whitespace">   </span><span class="mi">2734</span>     <span class="n">sqnorm</span> <span class="o">=</span> <span class="n">x_real</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x_real</span><span class="p">)</span> <span class="o">+</span> <span class="n">x_imag</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x_imag</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">2735</span> <span class="k">else</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">2736</span>     <span class="n">sqnorm</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">2737</span> <span class="n">ret</span> <span class="o">=</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">sqnorm</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">2738</span> <span class="k">if</span> <span class="n">keepdims</span><span class="p">:</span>

<span class="ne">KeyboardInterrupt</span>: 
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./courses\PR\Classification"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="SVDD.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Support Vector Data Description (SVDD)</p>
      </div>
    </a>
    <a class="right-next"
       href="Fisherclassifier.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Fihser Classifier</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-support-vector-machine-svm">What is a Support Vector Machine (SVM)?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examining-some-common-concepts-in-the-support-vector-machine-algorithm">Examining Some Common Concepts in the Support Vector Machine Algorithm</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-support-vector-machine">Linear Support Vector Machine</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-relationships-and-norm-calculations">Vector Relationships and Norm Calculations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hard-margin">Hard-margin</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-insensitive-loss-function">The ε-insensitive loss function:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#characteristics-of-the-solution">Characteristics of the Solution:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#soft-margin">Soft-margin</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#non-linear-support-vector-machines">Non-linear Support Vector Machines</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-class-support-vector-classifier-svc">Multi-Class Support Vector Classifier (SVC)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-vs-rest-method">One-vs-Rest Method</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-vs-one-method">One-vs-One Method</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-and-limitations-of-svm">Challenges and Limitations of SVM</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#homework-write-svm-with-an-arbitrary-loss-function">HomeWork: Write SVM with an arbitrary loss function</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Dr.Hadi Sadoghi Yazdi
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024 Pattern Recognition Lab.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/bootstrap.js?digest=3ee479438cf8b5e0d341"></script>
<script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=3ee479438cf8b5e0d341"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>