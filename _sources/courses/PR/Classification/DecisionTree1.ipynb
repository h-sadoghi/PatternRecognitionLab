{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Induction in Decision Trees\n",
    "\n",
    "**This project is worth 2 points out of 20**\n",
    "\n",
    "Complete with details and corrections in native English. Additionally, include any necessary innovations for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Induction in Decision Trees: Mathematical Explanation and Overview\n",
    "\n",
    "**Decision Trees** are a type of supervised learning algorithm used for classification and regression tasks. The structure of a decision tree consists of nodes that represent tests on features, branches that represent the outcome of tests, and leaf nodes that represent class labels or continuous values.\n",
    "\n",
    "### 1. **Basic Concepts:**\n",
    "\n",
    "- **Root Node**: The topmost node in a decision tree, representing the entire dataset.\n",
    "- **Leaf Nodes**: Terminal nodes that represent the output (class label or regression value).\n",
    "- **Branches**: Paths from one node to another based on feature values.\n",
    "\n",
    "### 2. **Splitting Criteria:**\n",
    "\n",
    "The goal of a decision tree is to partition the data such that the resulting subgroups are as pure as possible. Several criteria are used to determine the best splits:\n",
    "\n",
    "- **Gini Index**: Measures the impurity of a node. The lower the Gini index, the purer the node.\n",
    "  \n",
    "  $$\n",
    "  \\text{Gini}(t) = 1 - \\sum_{i=1}^C p_i^2\n",
    "  $$\n",
    "\n",
    "  where $ p_i $ is the probability of class $ i $ at node $ t $, and $ C $ is the number of classes.\n",
    "\n",
    "- **Information Gain**: Measures the reduction in entropy before and after a split. Entropy is a measure of impurity in the data.\n",
    "\n",
    "  $$\n",
    "  \\text{Entropy}(S) = -\\sum_{i=1}^C p_i \\log_2(p_i)\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  \\text{Information Gain}(S, A) = \\text{Entropy}(S) - \\sum_{v \\in \\text{Values}(A)} \\frac{|S_v|}{|S|} \\text{Entropy}(S_v)\n",
    "  $$\n",
    "\n",
    "  where $ S $ is the set of instances, $ A $ is the attribute, and $ S_v $ is the subset of $ S $ for which attribute $ A $ has value $ v $.\n",
    "\n",
    "- **Gain Ratio**: Adjusts information gain by taking into account the intrinsic information of a split to avoid bias towards attributes with many values.\n",
    "\n",
    "$$\n",
    "\\text{Gain Ratio}(A) = \\frac{\\text{Information Gain}(S, A)}{\\text{Split Information}(A)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Split Information}(A) = -\\sum_{v \\in \\text{Values}(A)} \\frac{|S_v|}{|S|} \\log_2\\left(\\frac{|S_v|}{|S|}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "### 3. **Tree Building Process:**\n",
    "\n",
    "- **Select the Best Feature to Split**: Using criteria like Gini Index, Information Gain, or Reduction in Variance, determine the feature that best splits the data at each node.\n",
    "- **Split the Dataset**: Partition the dataset based on the selected feature.\n",
    "- **Recursively Apply the Process**: Apply the splitting process recursively to the subsets of data created from the previous step.\n",
    "- **Stop Criteria**: The recursion stops when a stopping criterion is met, such as:\n",
    "   - All instances in a node belong to the same class.\n",
    "   - Maximum tree depth is reached.\n",
    "   - Minimum number of samples per node is reached.\n",
    "   - No further information gain is achieved.\n",
    "\n",
    "### 4. **Pruning:**\n",
    "\n",
    "Pruning is a technique used to avoid overfitting by trimming the branches of the tree that have little importance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
