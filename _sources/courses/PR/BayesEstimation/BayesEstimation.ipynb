{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Estimation\n",
    "\n",
    "Bayesian estimation is a statistical technique that applies Bayes' rule. It combines prior knowledge (prior distribution) with data (likelihood) to form an updated knowledge (posterior distribution). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes' Rule\n",
    "\n",
    "$$\n",
    "P(\\theta | \\mathbf{D}) = \\frac{P(\\mathbf{D} | \\theta) P(\\theta)}{P(\\mathbf{D})}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $ \\theta $ is the parameter or hypothesis.\n",
    "- $ \\mathbf{D} $ is the observed data (Data set).\n",
    "- $ P(\\theta | \\mathbf{D}) $ is the posterior probability, the probability of the parameter given the data.\n",
    "- $ P(\\mathbf{D} | \\theta) $ is the likelihood, the probability of the data given the parameter.\n",
    "- $ P(\\theta) $ is the prior probability, the initial belief about the parameter before observing the data.\n",
    "- $ P(\\mathbf{D}) $ is the total probability of the data.\n",
    "\n",
    "### Prior Distribution\n",
    "\n",
    "The prior distribution $ P(\\theta) $ represents our beliefs about the parameter $ \\theta $ before observing any data. The choice of the prior can be subjective or  objective.\n",
    "\n",
    "**Source of Prior Knowledge**\n",
    "The notion that prior knowledge must be received from a divine or transcendent source, such as God, touches on epistemological and metaphysical questions:\n",
    "\n",
    "\n",
    "#### Epistemological and Metaphysical Perspective:\n",
    "From an epistemological and metaphysical standpoint, prior knowledge can be seen as originating from various sources, including \n",
    "- intuition\n",
    "- previous experience\n",
    "- expert opinion\n",
    "- theoretical consideration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk minimization in the Bayesian perspective\n",
    "\n",
    "Steps for Bayesian Risk Minimization\n",
    "\n",
    "**Determine the Posterior Distribution**: Compute the posterior distribution $p(\\theta | X)$ using Bayes' Rule.\n",
    "\n",
    "$$\n",
    "p(\\theta | X) = \\frac{p(X | \\theta) p(\\theta)}{p(X)}\n",
    "$$\n",
    "\n",
    "**Define the Loss Function**: Choose an appropriate loss function $L(\\theta, \\theta^{*})$ based on the problem context.\n",
    "\n",
    "**Compute the Expected Posterior Loss**: Integrate the loss function over the posterior distribution to get the expected loss for each possible action.\n",
    "\n",
    "$$\n",
    "R(\\theta^{*} | X) = \\int L(\\theta, \\theta^{*}) p(\\theta | X) \\, d\\theta\n",
    "$$\n",
    "\n",
    "**Minimize the Expected Loss**: Select the action $a^*$ that minimizes the expected posterior loss.\n",
    "\n",
    "$$\n",
    "\\theta^{optimal} = \\arg\\min_\\theta R(\\theta | X)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood\n",
    "\n",
    "The likelihood function $ P(\\mathbf{D} | \\theta) $ describes how likely the observed data $ \\mathbf{D} $ is for different values of the parameter $ \\theta $. It is a function of $ \\theta $ given the data $ \\mathbf{D} $.\n",
    "\n",
    "### Posterior Distribution\n",
    "\n",
    "The posterior distribution $ P(\\theta | \\mathbf{D}) $ combines the prior distribution and the likelihood to give the updated belief about the parameter $ \\theta $ after observing the data $ \\mathbf{D} $.\n",
    "\n",
    "### Marginal Likelihood (Evidence)\n",
    "\n",
    "The marginal likelihood $ P(\\mathbf{D}) $ is the normalizing constant ensuring that the posterior distribution is a proper probability distribution. It is computed as:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{D}) = \\int P(\\mathbf{D} | \\theta) P(\\theta) \\, d\\theta\n",
    "$$\n",
    "\n",
    "### Bayesian Estimator\n",
    "\n",
    "The Bayesian estimator is a point estimate derived from the posterior distribution. Common Bayesian estimators include:\n",
    "\n",
    "- **Maximum A Posteriori (MAP) Estimate**: The mode of the posterior distribution, which maximizes $ P(\\theta | \\mathbf{D}) $.\n",
    "\n",
    "$$ \\hat{\\theta}_{MAP} = \\arg\\max_\\theta P(\\theta | \\mathbf{D}) $$\n",
    "\n",
    "Follow for proof it.\n",
    "\n",
    "### Zero-One Loss\n",
    "Sure, let's substitute $ L(\\theta, \\theta^{*}) = 1 - \\delta(\\theta, \\theta^{*}) $ into the given integral expression for $ R(\\theta^{*} | X) $.\n",
    "\n",
    "Given:\n",
    "\n",
    "$$\n",
    "R(\\theta^{*} | X) = \\int L(\\theta, \\theta^{*}) p(\\theta | X) \\, d\\theta\n",
    "$$\n",
    "\n",
    "Substitute $ L(\\theta, \\theta^{*}) = 1 - \\delta(\\theta, \\theta^{*}) $:\n",
    "\n",
    "$$\n",
    "R(\\theta^{*} | X) = \\int (1 - \\delta(\\theta, \\theta^{*})) p(\\theta | X) \\, d\\theta\n",
    "$$\n",
    "\n",
    "Now, let's break this down into two separate integrals:\n",
    "\n",
    "$$\n",
    "R(\\theta^{*} | X) = \\int p(\\theta | X) \\, d\\theta - \\int \\delta(\\theta, \\theta^{*}) p(\\theta | X) \\, d\\theta\n",
    "$$\n",
    "\n",
    "The first term is the integral of the probability density function $ p(\\theta | X) $ over the entire domain of $ \\theta $, which is equal to 1 (since it is a probability density function):\n",
    "\n",
    "$$\n",
    "\\int p(\\theta | X) \\, d\\theta = 1\n",
    "$$\n",
    "\n",
    "The second term involves the Kronecker delta function, which is 1 if $ \\theta = \\theta^{*} $ and 0 otherwise. Thus, the integral simplifies to evaluating $ p(\\theta | X) $ at $ \\theta = \\theta^{*} $:\n",
    "\n",
    "$$\n",
    "\\int \\delta(\\theta, \\theta^{*}) p(\\theta | X) \\, d\\theta = p(\\theta^{*} | X)\n",
    "$$\n",
    "\n",
    "Putting it all together:\n",
    "\n",
    "$$\n",
    "R(\\theta^{*} | X) = 1 - p(\\theta^{*} | X)\n",
    "$$\n",
    "\n",
    "So the substituted expression is:\n",
    "\n",
    "$$\n",
    "R(\\theta^{*} | X) = 1 - p(\\theta^{*} | X)\n",
    "$$\n",
    "\n",
    "**Bayesian Risk Minimization**\n",
    "\n",
    "To find the value of $\\theta^*$ that minimizes $R(\\theta^* | X)$, we can set up the optimization problem as follows:\n",
    "\n",
    "$$ \\theta^* =\\arg \\min_{\\theta} R(\\theta | X) $$\n",
    "\n",
    "Since we have:\n",
    "\n",
    "$$ R(\\theta^* | X) = 1 - p(\\theta^* | X) $$\n",
    "\n",
    "we want to minimize $1 - p(\\theta^* | X)$. Minimizing this expression is equivalent to maximizing $p(\\theta^* | X)$ because 1 is a constant and does not affect the optimization.\n",
    "\n",
    "Therefore, we have:\n",
    "\n",
    "$$\n",
    "\\arg \\min_{\\theta^*} (1 - p(\\theta^* | X)) = \\arg \\max_{\\theta^*} p(\\theta^* | X)\n",
    "$$\n",
    "\n",
    "So the value of $\\theta^*$ that minimizes $R(\\theta^* | X)$ is the same as the value of $\\theta^*$ that maximizes $p(\\theta^* | X)$:\n",
    "\n",
    "$$ \\theta^* = \\arg \\max_{\\theta} p(\\theta | X) \n",
    "$$\n",
    "\n",
    "This is often referred to as the maximum a posteriori (MAP) estimate in Bayesian inference.\n",
    "\n",
    "\n",
    "- **Posterior Mean**: The expected value of the posterior distribution.\n",
    "\n",
    "$$ \\hat{\\theta}_{mean} = \\mathbb{E}[\\theta | \\mathbf{D}] = \\int \\theta P(\\theta | \\mathbf{D}) \\, d\\theta $$\n",
    "\n",
    "### Square Loss\n",
    "Sure, let's substitute $ L(\\theta, \\theta^{*}) = |\\theta-\\theta^{*}|^2 $ into the given integral expression for $ R(\\theta^{*} | X) $.\n",
    "\n",
    "Given:\n",
    "\n",
    "$$\n",
    "R(\\theta^{*} | X) = \\int  |\\theta-\\theta^{*}|^2 p(\\theta | X) \\, d\\theta\n",
    "$$\n",
    "\n",
    "To find the partial derivative of $ R(\\theta^* | X) = \\int  |\\theta - \\theta^*|^2 p(\\theta | X) \\, d\\theta $ with respect to $\\theta^*$, we can follow these steps:\n",
    "\n",
    "**Define the function $ R(\\theta^* | X) $:**\n",
    "\n",
    "$$\n",
    "R(\\theta^* | X) = \\int  |\\theta - \\theta^*|^2 p(\\theta | X) \\, d\\theta\n",
    "$$\n",
    "\n",
    "**Compute the partial derivative relative to $\\theta^*$:**\n",
    "   Let's denote $ |\\theta - \\theta^*|^2 $ as the integrand function $ f(\\theta, \\theta^*) = |\\theta - \\theta^*|^2 $.\n",
    "\n",
    "**Use the Leibniz integral rule:**\n",
    "   The Leibniz rule for differentiation under the integral sign allows us to differentiate an integral with respect to a parameter:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta^*} R(\\theta^* | X) = \\frac{\\partial}{\\partial \\theta^*} \\int |\\theta - \\theta^*|^2 p(\\theta | X) \\, d\\theta\n",
    "$$\n",
    "\n",
    "**Apply the derivative inside the integral:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta^*} R(\\theta^* | X) = \\int (-2\\theta + 2\\theta^*) p(\\theta | X) \\, d\\theta\n",
    "$$\n",
    "\n",
    "**Simplify the expression:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta^*} R(\\theta^* | X) = 2 \\int (\\theta^* - \\theta) p(\\theta | X) \\, d\\theta\n",
    "$$\n",
    "\n",
    "Thus, the partial derivative of $ R(\\theta^* | X) $ with respect to $\\theta^*$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta^*} R(\\theta^* | X) = 2 \\int (\\theta^* - \\theta) p(\\theta | X) \\, d\\theta=0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_{mean} = \\mathbb{E}[\\theta | \\mathbf{X}] = \\int \\theta P(\\theta | \\mathbf{X}) \\, d\\theta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correntropy Loss\n",
    "\n",
    "Let's continue by substituting $L(\\theta, \\theta^{*}) = 1 - \\exp\\left(\\frac{-|\\theta - \\theta^*|^2}{2\\sigma^2}\\right)$ into the integral expression for $ R(\\theta^* | X) $.\n",
    "\n",
    "Given:\n",
    "\n",
    "$$\n",
    "R(\\theta^* | X) = \\int L(\\theta, \\theta^{*}) p(\\theta | X) \\, d\\theta\n",
    "$$\n",
    "\n",
    "$$\n",
    "R(\\theta^* | X) = \\int \\left(1 - \\exp\\left(\\frac{-|\\theta - \\theta^*|^2}{2\\sigma^2}\\right)\\right) p(\\theta | X) \\, d\\theta\n",
    "$$\n",
    "\n",
    "Now, let's differentiate $ R(\\theta^* | X) $ with respect to $ \\theta^* $.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta^*} R(\\theta^* | X) = \\frac{\\partial}{\\partial \\theta^*} \\int \\left(1 - \\exp\\left(\\frac{-|\\theta - \\theta^*|^2}{2\\sigma^2}\\right)\\right) p(\\theta | X) \\, d\\theta\n",
    "$$\n",
    "\n",
    "Since $ 1 $ is a constant and does not depend on $ \\theta^* $, we only need to differentiate the exponential term:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta^*} R(\\theta^* | X) = -\\int \\frac{\\partial}{\\partial \\theta^*} \\exp\\left(\\frac{-|\\theta - \\theta^*|^2}{2\\sigma^2}\\right) p(\\theta | X) \\, d\\theta\n",
    "$$\n",
    "\n",
    "**Differentiate the exponential term:**\n",
    "\n",
    "To differentiate $\\exp\\left(\\frac{-|\\theta - \\theta^*|^2}{2\\sigma^2}\\right)$ with respect to $\\theta^*$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta^*} \\exp\\left(\\frac{-|\\theta - \\theta^*|^2}{2\\sigma^2}\\right) = \\exp\\left(\\frac{-|\\theta - \\theta^*|^2}{2\\sigma^2}\\right) \\cdot \\frac{\\partial}{\\partial \\theta^*} \\left(\\frac{-|\\theta - \\theta^*|^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "**Differentiate the inner term:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta^*} \\left(\\frac{-|\\theta - \\theta^*|^2}{2\\sigma^2}\\right) = \\frac{-1}{2\\sigma^2} \\cdot 2(\\theta^* - \\theta) = \\frac{-(\\theta^* - \\theta)}{\\sigma^2}\n",
    "$$\n",
    "\n",
    "Putting it all together:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta^*} \\exp\\left(\\frac{-|\\theta - \\theta^*|^2}{2\\sigma^2}\\right) = \\exp\\left(\\frac{-|\\theta - \\theta^*|^2}{2\\sigma^2}\\right) \\cdot \\frac{-(\\theta^* - \\theta)}{\\sigma^2}\n",
    "$$\n",
    "\n",
    "**Substitute back into the integral:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta^*} R(\\theta^* | X) = -\\int \\exp\\left(\\frac{-|\\theta - \\theta^*|^2}{2\\sigma^2}\\right) \\cdot \\frac{-(\\theta^* - \\theta)}{\\sigma^2} p(\\theta | X) \\, d\\theta\n",
    "$$\n",
    "\n",
    "**Simplify the expression:**\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta^*} R(\\theta^* | X) = \\frac{1}{\\sigma^2} \\int (\\theta^* - \\theta) \\exp\\left(\\frac{-|\\theta - \\theta^*|^2}{2\\sigma^2}\\right) p(\\theta | X) \\, d\\theta\n",
    "$$\n",
    "\n",
    "Thus, the partial derivative of $ R(\\theta^* | X) $ with respect to $\\theta^*$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta^*} R(\\theta^* | X) = \\frac{1}{\\sigma^2} \\int (\\theta^* - \\theta) \\exp\\left(\\frac{-|\\theta - \\theta^*|^2}{2\\sigma^2}\\right) p(\\theta | X) \\, d\\theta\n",
    "$$\n",
    "\n",
    "Let's follow the given instructions to simplify and solve for $\\theta^*$.\n",
    "\n",
    "Given:\n",
    "\n",
    "$$\n",
    "\\exp\\left(\\frac{-|\\theta - \\theta^*|^2}{2\\sigma^2}\\right) = w(\\theta, \\theta^*)\n",
    "$$\n",
    "\n",
    "We assume $w(\\theta, \\theta^*)$ is fixed. Now, the partial derivative of $ R(\\theta^* | X) $ with respect to $\\theta^*$ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\theta^*} R(\\theta^* | X) = \\frac{1}{\\sigma^2} \\int (\\theta^* - \\theta) w(\\theta, \\theta^*) p(\\theta | X) \\, d\\theta\n",
    "$$\n",
    "\n",
    "Setting the derivative equal to zero to find the critical points:\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\sigma^2} \\int (\\theta^* - \\theta) w(\\theta, \\theta^*) p(\\theta | X) \\, d\\theta = 0\n",
    "$$\n",
    "\n",
    "Since $\\frac{1}{\\sigma^2}$ is a constant and $\\sigma^2 \\neq 0$, we can ignore it in the equation:\n",
    "\n",
    "$$\n",
    "\\int (\\theta^* - \\theta) w(\\theta, \\theta^*) p(\\theta | X) \\, d\\theta = 0\n",
    "$$\n",
    "\n",
    "Now, factor out $\\theta^*$ from the integral:\n",
    "\n",
    "$$\n",
    "\\theta^* \\int w(\\theta, \\theta^*) p(\\theta | X) \\, d\\theta - \\int \\theta w(\\theta, \\theta^*) p(\\theta | X) \\, d\\theta = 0\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$\n",
    "\\theta^* = \\frac{\\int \\theta w(\\theta, \\theta^*) p(\\theta | X) \\, d\\theta}{\\int w(\\theta, \\theta^*) p(\\theta | X) \\, d\\theta}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Bayesian Estimation for Gaussian Distribution\n",
    "\n",
    "Let's consider a simple example where the observed data $ \\mathbf{D} = \\{x_1, x_2, \\ldots, x_n\\} $ is assumed to come from a Gaussian distribution with unknown mean $ \\mu $ and known variance $ \\sigma^2 $.\n",
    "\n",
    "#### Prior Distribution\n",
    "\n",
    "Assume a normal prior for the mean $ \\mu $:\n",
    "\n",
    "$$ \\mu \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2) $$\n",
    "\n",
    "#### Likelihood\n",
    "\n",
    "The likelihood of the independent observed data given $ \\mu $ is:\n",
    "\n",
    "$$ P(\\mathbf{D} | \\mu) = P(\\mathbf{x_1,...,x_n} | \\mu)=\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\mathbf{x_1} | \\mu)*...*P(\\mathbf{x_2} | \\mu)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\prod_{i=1}^n \\mathcal{N}(x_i | \\mu, \\sigma^2)\n",
    "$$\n",
    "\n",
    "**Bayes' Rule:**\n",
    "\n",
    "$$\n",
    "P(\\mu | \\mathbf{D}) = \\frac{P(\\mathbf{D} | \\mu) P(\\mu)}{P(\\mathbf{D})}\n",
    "$$\n",
    "\n",
    "**Prior Distribution $P(\\mu)$:**\n",
    "\n",
    "   Assume the prior distribution of $\\mu$ is normal:\n",
    "\n",
    "$$\n",
    "\\mu \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)\n",
    "$$\n",
    "\n",
    "   So,\n",
    "\n",
    "$$\n",
    "P(\\mu) = \\frac{1}{\\sqrt{2\\pi\\sigma_0^2}} \\exp\\left(-\\frac{(\\mu - \\mu_0)^2}{2\\sigma_0^2}\\right)\n",
    "$$\n",
    "\n",
    "**Likelihood $P(\\mathbf{D} | \\mu)$:**\n",
    "\n",
    "Assume the data $\\mathbf{D} = \\{x_1, x_2, \\ldots, x_n\\}$ are i.i.d. samples from a normal distribution with mean $\\mu$ and known variance $\\sigma^2$:\n",
    "\n",
    "$$\n",
    "x_i \\sim \\mathcal{N}(\\mu, \\sigma^ i)\n",
    "$$\n",
    "\n",
    "follows $\\mathcal{N}(\\mu, \\sigma^2)$.\n",
    "\n",
    "The likelihood function is:\n",
    "\n",
    "$$\n",
    "P(\\mathbf{D} | \\mu) = \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^n \\exp\\left(-\\sum_{i=1}^n \\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "Simplify further using the sample mean $\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i$:\n",
    "\n",
    "\n",
    "**Posterior Distribution $P(\\mu | \\mathbf{D})$:**\n",
    "\n",
    "Combine the prior and the likelihood:\n",
    "\n",
    "$$\n",
    "P(\\mu | \\mathbf{D}) \\propto P(\\mathbf{D} | \\mu) P(\\mu)\n",
    "$$\n",
    "\n",
    "## HomeWork : Continue and obtain the following\n",
    "**Resulting Posterior Distribution:**\n",
    "\n",
    "   The resulting posterior distribution is a normal distribution with:\n",
    "\n",
    "   - Mean: $\\mu_{\\text{post}} = \\frac{\\sigma^2\\mu_0 + n\\sigma_0^2\\bar{x}}{\\sigma^2 + n\\sigma_0^2}$\n",
    "   - Variance: $\\sigma^2_{\\text{post}} = \\frac{\\sigma^2 \\sigma_0^2}{\\sigma^2 + n\\sigma_0^2}$\n",
    "\n",
    "Therefore, the posterior distribution of $\\mu$ given the data $\\mathbf{D}$ is:\n",
    "\n",
    "$$\n",
    "\\mu | \\mathbf{D} \\sim \\mathcal{N}\\left(\\frac{\\sigma^2 \\mu_0 + n \\sigma_0^2 \\bar{x}}{\\sigma^2 + n \\sigma_0^2}, \\frac{\\sigma^2 \\sigma_0^2}{\\sigma^2 + n \\sigma_0^2}\\right)\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
