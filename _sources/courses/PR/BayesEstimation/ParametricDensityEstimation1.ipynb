{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametric Density Estimation\n",
    "Parametric density estimation involves assuming that the underlying data distribution follows a specific parametric family of distributions and then estimating the parameters of that distribution from the given data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some parametric models and their parameters\n",
    "\n",
    "1. **Normal Distribution** ($\\mathcal{N}(\\mu, \\sigma^2)$)\n",
    "   - **Parameters**: \n",
    "     - $\\mu$: Mean of the distribution\n",
    "     - $\\sigma^2$: Variance of the distribution\n",
    "\n",
    "\n",
    "$$\n",
    "f(x|\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(x-\\mu)^2}{2\\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "**Where:** Widely used in natural and social sciences.\n",
    "\n",
    "**Why:** Many phenomena naturally follow a normal distribution due to the Central Limit Theorem, which states that the sum of a large number of independent, identically distributed variables will be approximately normally distributed.\n",
    "\n",
    "**Specific Applications and Contexts:** \n",
    "**Natural Sciences:** Heights, weights, and test scores.\n",
    "**Social Sciences:** IQ scores, measurement errors.\n",
    "\n",
    "\n",
    "2. **Exponential Distribution** ($\\text{Exp}(\\lambda)$)\n",
    "   - **Parameter**: \n",
    "     - $\\lambda$: Rate parameter (inverse of the mean)\n",
    "   \n",
    "$$\n",
    "f(x|\\lambda) = \\lambda \\exp(-\\lambda x), \\quad x \\geq 0\n",
    "$$\n",
    "\n",
    "**Where:** Reliability engineering, queueing theory, and survival analysis.\n",
    "**Why:** Models the time between events in a Poisson process, such as the lifespan of an electronic component or the time until the next customer arrives.\n",
    "\n",
    "**Specific Applications and Contexts:**\n",
    "**Reliability Engineering:** Time until failure of a machine component.\n",
    "**Queueing Theory:** Time between arrivals of customers at a service point.\n",
    "\n",
    "3. **Binomial Distribution** ($\\text{Bin}(n, p)$)\n",
    "   - **Parameters**: \n",
    "     - $n$: Number of trials\n",
    "     - $p$: Probability of success in each trial\n",
    "   \n",
    "\n",
    "$$\n",
    "P(X=k|n,p) = \\binom{n}{k} p^k (1-p)^{n-k}, \\quad k = 0, 1, \\ldots, n\n",
    "$$\n",
    "\n",
    "**Where:** Quality control, clinical trials.\n",
    "**Why:** Models the number of successes in a fixed number of independent Bernoulli trials, such as the number of defective items in a batch or the number of patients responding to a treatment.\n",
    "\n",
    "\n",
    "4. **Poisson Distribution** ($\\text{Pois}(\\lambda)$)\n",
    "   - **Parameter**: \n",
    "     - $\\lambda$: Average number of events in a given interval\n",
    "\n",
    "$$\n",
    "P(X=k|\\lambda) = \\frac{\\lambda^k e^{-\\lambda}}{k!}, \\quad k = 0, 1, 2, \\ldots\n",
    "$$\n",
    "\n",
    "   - **Usage**: Models the number of events occurring within a fixed interval of time or space.\n",
    "\n",
    "5. **Gamma Distribution** ($\\text{Gamma}(\\alpha, \\beta)$)\n",
    "   - **Parameters**: \n",
    "     - $\\alpha$: Shape parameter\n",
    "     - $\\beta$: Rate parameter (inverse of scale)\n",
    "\n",
    "$$\n",
    "f(x|\\alpha,\\beta) = \\frac{\\beta^\\alpha x^{\\alpha-1} e^{-\\beta x}}{\\Gamma(\\alpha)}, \\quad x > 0\n",
    "$$\n",
    "\n",
    "   - **Usage**: Often used in Bayesian statistics as prior distributions.\n",
    "\n",
    "6. **Beta Distribution** ($\\text{Beta}(\\alpha, \\beta)$)\n",
    "   - **Parameters**: \n",
    "     - $\\alpha$: Shape parameter\n",
    "     - $\\beta$: Shape parameter\n",
    "\n",
    "$$\n",
    "f(x|\\alpha,\\beta) = \\frac{x^{\\alpha-1} (1-x)^{\\beta-1}}{B(\\alpha,\\beta)}, \\quad 0 < x < 1\n",
    "$$\n",
    "\n",
    "   - **Usage**: Commonly used to model random variables that are bounded between 0 and 1.\n",
    "\n",
    "7. **Weibull Distribution** ($\\text{Weibull}(k, \\lambda)$)\n",
    "   - **Parameters**: \n",
    "     - $k$: Shape parameter\n",
    "     - $\\lambda$: Scale parameter\n",
    "\n",
    "$$\n",
    "f(x|k,\\lambda) = \\frac{k}{\\lambda} \\left(\\frac{x}{\\lambda}\\right)^{k-1} \\exp\\left(-\\left(\\frac{x}{\\lambda}\\right)^k\\right), \\quad x \\geq 0\n",
    "$$\n",
    "\n",
    "   - **Usage**: Used in reliability engineering and failure analysis.\n",
    "\n",
    "8. **Log-Normal Distribution** ($\\text{LogNorm}(\\mu, \\sigma^2)$)\n",
    "   - **Parameters**: \n",
    "     - $\\mu$: Mean of the log of the variable\n",
    "     - $\\sigma^2$: Variance of the log of the variable\n",
    "\n",
    "$$\n",
    "f(x|\\mu,\\sigma^2) = \\frac{1}{x\\sigma\\sqrt{2\\pi}} \\exp\\left( -\\frac{(\\log x - \\mu)^2}{2\\sigma^2} \\right), \\quad x > 0\n",
    "$$\n",
    "\n",
    "   - **Usage**: Models data that are positively skewed.\n",
    "\n",
    "9. **Chi-Square Distribution** ($\\chi^2(k)$)\n",
    "   - **Parameter**: \n",
    "     - $k$: Degrees of freedom\n",
    "\n",
    "$$\n",
    "f(x|k) = \\frac{1}{2^{k/2} \\Gamma(k/2)} x^{k/2-1} e^{-x/2}, \\quad x \\geq 0\n",
    "$$\n",
    "\n",
    "   - **Usage**: Commonly used in hypothesis testing and confidence interval estimation.\n",
    "\n",
    "10. **t-Distribution** ($t(\\nu)$)\n",
    "   - **Parameter**: \n",
    "     - $\\nu$: Degrees of freedom\n",
    "\n",
    "$$\n",
    "f(x|\\nu) = \\frac{\\Gamma\\left(\\frac{\\nu+1}{2}\\right)}{\\sqrt{\\nu\\pi} \\Gamma\\left(\\frac{\\nu}{2}\\right)} \\left(1 + \\frac{x^2}{\\nu}\\right)^{-\\frac{\\nu+1}{2}}\n",
    "$$\n",
    "\n",
    "![DrawDistribution](../BayesEstimation/BayesEstimationImages/DrawParametricDistribution1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation of Density Parameters\n",
    "We estimate parameters of density by Risk minimization in the Bayesian perspective.\n",
    "\n",
    "### Steps for Bayesian Risk Minimization\n",
    "\n",
    "1. **Determine the Posterior Distribution**: Compute the posterior distribution $p(\\theta | X)$ using Bayes' theorem.\n",
    "\n",
    "$$\n",
    "p(\\theta | X) = \\frac{p(X | \\theta) p(\\theta)}{p(X)}\n",
    "$$\n",
    "\n",
    "2. **Define the Loss Function**: Choose an appropriate loss function $L(\\theta, \\theta^{*})$ based on the problem context.\n",
    "\n",
    "3. **Compute the Expected Posterior Loss**: Integrate the loss function over the posterior distribution to get the expected loss for each possible action.\n",
    "\n",
    "$$\n",
    "R(\\theta^{*} | X) = \\int L(\\theta, \\theta^{*}) p(\\theta | X) \\, d\\theta\n",
    "$$\n",
    "\n",
    "4. **Minimize the Expected Loss**: Select the action $a^*$ that minimizes the expected posterior loss.\n",
    "\n",
    "$$\n",
    "\\theta^{optimal} = \\arg\\min_\\theta R(\\theta | X)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-One Loss\n",
    "Sure, let's substitute $ L(\\theta, \\theta^{*}) = 1 - \\delta(\\theta, \\theta^{*}) $ into the given integral expression for $ R(\\theta^{*} | X) $.\n",
    "\n",
    "Given:\n",
    "\n",
    "$$\n",
    "R(\\theta^{*} | X) = \\int L(\\theta, \\theta^{*}) p(\\theta | X) \\, d\\theta\n",
    "$$\n",
    "\n",
    "Substitute $ L(\\theta, \\theta^{*}) = 1 - \\delta(\\theta, \\theta^{*}) $:\n",
    "\n",
    "$$\n",
    "R(\\theta^{*} | X) = \\int (1 - \\delta(\\theta, \\theta^{*})) p(\\theta | X) \\, d\\theta\n",
    "$$\n",
    "\n",
    "Now, let's break this down into two separate integrals:\n",
    "\n",
    "$$\n",
    "R(\\theta^{*} | X) = \\int p(\\theta | X) \\, d\\theta - \\int \\delta(\\theta, \\theta^{*}) p(\\theta | X) \\, d\\theta\n",
    "$$\n",
    "\n",
    "The first term is the integral of the probability density function $ p(\\theta | X) $ over the entire domain of $ \\theta $, which is equal to 1 (since it is a probability density function):\n",
    "\n",
    "$$\n",
    "\\int p(\\theta | X) \\, d\\theta = 1\n",
    "$$\n",
    "\n",
    "The second term involves the Kronecker delta function, which is 1 if $ \\theta = \\theta^{*} $ and 0 otherwise. Thus, the integral simplifies to evaluating $ p(\\theta | X) $ at $ \\theta = \\theta^{*} $:\n",
    "\n",
    "$$\n",
    "\\int \\delta(\\theta, \\theta^{*}) p(\\theta | X) \\, d\\theta = p(\\theta^{*} | X)\n",
    "$$\n",
    "\n",
    "Putting it all together:\n",
    "\n",
    "$$\n",
    "R(\\theta^{*} | X) = 1 - p(\\theta^{*} | X)\n",
    "$$\n",
    "\n",
    "So the substituted expression is:\n",
    "\n",
    "$$\n",
    "R(\\theta^{*} | X) = 1 - p(\\theta^{*} | X)\n",
    "$$\n",
    "\n",
    "**Bayesian Risk Minimization**\n",
    "\n",
    "To find the value of $\\theta^*$ that minimizes $R(\\theta^* | X)$, we can set up the optimization problem as follows:\n",
    "\n",
    "$$ \\arg \\min_{\\theta^*} R(\\theta^* | X) $$\n",
    "\n",
    "Since we have:\n",
    "\n",
    "$$ R(\\theta^* | X) = 1 - p(\\theta^* | X) $$\n",
    "\n",
    "we want to minimize $1 - p(\\theta^* | X)$. Minimizing this expression is equivalent to maximizing $p(\\theta^* | X)$ because 1 is a constant and does not affect the optimization.\n",
    "\n",
    "Therefore, we have:\n",
    "\n",
    "$$ \\arg \\min_{\\theta^*} (1 - p(\\theta^* | X)) = \\arg \\max_{\\theta^*} p(\\theta^* | X) $$\n",
    "\n",
    "So the value of $\\theta^*$ that minimizes $R(\\theta^* | X)$ is the same as the value of $\\theta^*$ that maximizes $p(\\theta^* | X)$:\n",
    "\n",
    "$$ \\theta^* = \\arg \\max_{\\theta^*} p(\\theta^* | X) $$\n",
    "\n",
    "This is often referred to as the maximum a posteriori (MAP) estimate in Bayesian inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAP estimation of Normal distribution parameters\n",
    "To solve for $\\mu$ and $\\sigma^2$ given a dataset $\\{x_1, \\ldots, x_n\\}$ under the normal distribution $f(x|\\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(x-\\mu)^2}{2\\sigma^2} \\right)$, we will consider two cases: without prior knowledge (maximum likelihood estimation) and with prior knowledge.\n",
    "\n",
    "### Without Prior Knowledge (Maximum Likelihood Estimation)\n",
    "\n",
    "#### 1. Maximum Likelihood Estimation for $\\mu$ and $\\sigma^2$:\n",
    "\n",
    "The likelihood function for the normal distribution is:\n",
    "\n",
    "$$\n",
    "L(\\mu, \\sigma^2 | \\{x_1, \\ldots, x_n\\}) = \\prod_{i=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left( -\\frac{(x_i-\\mu)^2}{2\\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "The log-likelihood function is:\n",
    "\n",
    "$$\n",
    "\\log L(\\mu, \\sigma^2 | \\{x_1, \\ldots, x_n\\}) = -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "To find the maximum likelihood estimates (MLE) of $\\mu$ and $\\sigma^2$, we take the partial derivatives of the log-likelihood function with respect to $\\mu$ and $\\sigma^2$ and set them to zero.\n",
    "\n",
    "#### 2. Estimating $\\mu$:\n",
    "\n",
    "Taking the derivative with respect to $\\mu$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\mu} \\log L(\\mu, \\sigma^2 | \\{x_1, \\ldots, x_n\\}) = \\frac{1}{\\sigma^2} \\sum_{i=1}^{n} (x_i - \\mu) = 0\n",
    "$$\n",
    "\n",
    "Solving for $\\mu$:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} (x_i - \\mu) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "n\\mu = \\sum_{i=1}^{n} x_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n",
    "$$\n",
    "\n",
    "Thus, the MLE for $\\mu$ is the sample mean:\n",
    "\n",
    "$$\n",
    "\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n",
    "$$\n",
    "\n",
    "#### 3. Estimating $\\sigma^2$:\n",
    "\n",
    "Taking the derivative with respect to $\\sigma^2$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial}{\\partial \\sigma^2} \\log L(\\mu, \\sigma^2 | \\{x_1, \\ldots, x_n\\}) = -\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} \\sum_{i=1}^{n} (x_i - \\mu)^2 = 0\n",
    "$$\n",
    "\n",
    "Solving for $\\sigma^2$:\n",
    "\n",
    "$$\n",
    "-\\frac{n}{2\\sigma^2} + \\frac{1}{2\\sigma^4} \\sum_{i=1}^{n} (x_i - \\mu)^2 = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\sum_{i=1}^{n} (x_i - \\mu)^2}{\\sigma^4} = \\frac{n}{\\sigma^2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma^2 = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\mu)^2\n",
    "$$\n",
    "\n",
    "Thus, the MLE for $\\sigma^2$ is the sample variance:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\hat{\\mu})^2\n",
    "$$\n",
    "\n",
    "### With Prior Knowledge (Bayesian Estimation)\n",
    "\n",
    "For Bayesian estimation, we need to specify prior distributions for $\\mu$ and $\\sigma^2$. We'll assume conjugate priors to simplify the computations.\n",
    "\n",
    "#### 1. Prior for $\\mu$ and $\\sigma^2$:\n",
    "\n",
    "Assume the following priors:\n",
    "\n",
    "- $\\mu \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2)$\n",
    "- $\\sigma^2 \\sim \\text{Inverse-Gamma}(\\alpha, \\beta)$\n",
    "\n",
    "#### 2. Posterior Distribution:\n",
    "\n",
    "The joint posterior distribution of $\\mu$ and $\\sigma^2$ given the data is proportional to the product of the likelihood and the priors:\n",
    "\n",
    "$$\n",
    "p(\\mu, \\sigma^2 | \\{x_1, \\ldots, x_n\\}) \\propto L(\\mu, \\sigma^2 | \\{x_1, \\ldots, x_n\\}) \\cdot p(\\mu) \\cdot p(\\sigma^2)\n",
    "$$\n",
    "\n",
    "#### 3. Posterior for $\\mu$:\n",
    "\n",
    "Integrating out $\\sigma^2$, the posterior distribution for $\\mu$ given the data follows a normal distribution with updated parameters:\n",
    "\n",
    "$$\n",
    "\\mu | \\{x_1, \\ldots, x_n\\}, \\sigma^2 \\sim \\mathcal{N}\\left( \\frac{\\sigma_0^2 \\sum_{i=1}^{n} x_i + \\sigma^2 \\mu_0}{n \\sigma_0^2 + \\sigma^2}, \\frac{\\sigma_0^2 \\sigma^2}{n \\sigma_0^2 + \\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "#### 4. Posterior for $\\sigma^2$:\n",
    "\n",
    "The posterior distribution for $\\sigma^2$ given the data and $\\mu$ follows an inverse-gamma distribution with updated parameters:\n",
    "\n",
    "$$\n",
    "\\sigma^2 | \\{x_1, \\ldots, x_n\\} \\sim \\text{Inverse-Gamma}\\left( \\alpha + \\frac{n}{2}, \\beta + \\frac{1}{2} \\sum_{i=1}^{n} (x_i - \\mu)^2 \\right)\n",
    "$$\n",
    "\n",
    "### At a Glance\n",
    "\n",
    "- **Without Prior Knowledge (MLE):**\n",
    "  \n",
    "  $$\n",
    "  \\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n",
    "  $$\n",
    "  \n",
    "  $$\n",
    "  \\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\hat{\\mu})^2\n",
    "  $$\n",
    "\n",
    "- **With Prior Knowledge (Bayesian Estimation):**\n",
    "  - Posterior for $\\mu$:\n",
    "    \n",
    "    $$\n",
    "    \\mu | \\{x_1, \\ldots, x_n\\}, \\sigma^2 \\sim \\mathcal{N}\\left( \\frac{\\sigma_0^2 \\sum_{i=1}^{n} x_i + \\sigma^2 \\mu_0}{n \\sigma_0^2 + \\sigma^2}, \\frac{\\sigma_0^2 \\sigma^2}{n \\sigma_0^2 + \\sigma^2} \\right)\n",
    "    $$\n",
    "\n",
    "  - Posterior for $\\sigma^2$:\n",
    "    \n",
    "    $$\n",
    "    \\sigma^2 | \\{x_1, \\ldots, x_n\\} \\sim \\text{Inverse-Gamma}\\left( \\alpha + \\frac{n}{2}, \\beta + \\frac{1}{2} \\sum_{i=1}^{n} (x_i - \\mu)^2 \\right)$$\n",
    "\n",
    "### What is the Inverse-Gamma Distribution?\n",
    "\n",
    "The Inverse-Gamma distribution is a continuous probability distribution defined for positive real numbers. If a random variable $X$ follows an Inverse-Gamma distribution with shape parameter $\\alpha$ and scale parameter $\\beta$, it is denoted as:\n",
    "\n",
    "$$\n",
    "X \\sim \\text{Inverse-Gamma}(\\alpha, \\beta)\n",
    "$$\n",
    "\n",
    "The probability density function (PDF) of the Inverse-Gamma distribution is given by:\n",
    "\n",
    "$$\n",
    "f(x; \\alpha, \\beta) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{-(\\alpha + 1)} \\exp\\left(-\\frac{\\beta}{x}\\right)\n",
    "$$\n",
    "\n",
    "for $x > 0$, where $\\alpha > 0$ and $\\beta > 0$ are the shape and scale parameters, respectively, and $\\Gamma(\\alpha)$ is the gamma function.\n",
    "\n",
    "### Why Use the Inverse-Gamma Distribution?\n",
    "\n",
    "1. **Conjugacy**:\n",
    "   - In Bayesian statistics, conjugate priors are chosen because they simplify the computation of the posterior distribution. For the variance parameter $\\sigma^2$ of a normal distribution, the Inverse-Gamma distribution is conjugate to the normal likelihood. This means that if the prior for $\\sigma^2$ is an Inverse-Gamma distribution, the posterior distribution for $\\sigma^2$ will also be an Inverse-Gamma distribution. This conjugacy leads to analytically tractable forms for the posterior, making it easier to perform Bayesian inference.\n",
    "\n",
    "2. **Positive Support**:\n",
    "   - The Inverse-Gamma distribution is defined only for positive values, which is appropriate for variance parameters because variances must be positive.\n",
    "\n",
    "3. **Flexibility**:\n",
    "   - The Inverse-Gamma distribution is flexible and can model a wide range of prior beliefs about the variance parameter by adjusting its shape and scale parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: \n",
    "We have a dataset for student grades in a pattern recognition course. We need to find the mean (μ) and standard deviation (σ) from the normal distribution in two scenarios: with and without prior information. For μ, we generally assume the prior is $ N(10, 2) $. However, for σ, we only know that it falls within a range, with the mean approximately 1 and the maximum value approximately 3. Therefore, the first step is to find the inverse gamma distribution using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 1.6505255701202999, Beta: 0.6506534459879487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dr\\AppData\\Local\\Temp\\ipykernel_13928\\3248625107.py:19: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  alpha, beta = optimize.fsolve(equations, initial_guess)\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "import scipy.optimize as optimize\n",
    "\n",
    "# Target mean and extreme value\n",
    "mean_target = 1\n",
    "extreme_value = 3\n",
    "\n",
    "# Function to solve for alpha and beta\n",
    "def equations(params):\n",
    "    alpha, beta = params\n",
    "    mean_eq = beta / (alpha - 1) - mean_target\n",
    "    prob_eq = 1 - stats.invgamma.cdf(extreme_value, alpha, scale=beta) - 0.05  # 5% extreme value\n",
    "    return [mean_eq, prob_eq]\n",
    "\n",
    "# Initial guess\n",
    "initial_guess = [2, 1]\n",
    "\n",
    "# Solve the equations\n",
    "alpha, beta = optimize.fsolve(equations, initial_guess)\n",
    "\n",
    "print(f\"Alpha: {alpha}, Beta: {beta}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second step, we solve for μ and σ with and without prior information.\n",
    "\n",
    "## More Details\n",
    "\n",
    "Let's go through the calculations for estimating $\\mu$ and $\\sigma^2$ for the dataset $\\{0, 1, \\ldots, 20\\}$ with and without prior knowledge. The priors are given as:\n",
    "\n",
    "- $\\mu \\sim \\mathcal{N}(10, 2^2)$\n",
    "- $\\sigma^2 \\sim \\text{Inverse-Gamma}(1.6505255701202999, 0.6506534459879487)$\n",
    "\n",
    "### Without Prior Knowledge (Maximum Likelihood Estimation)\n",
    "\n",
    "Given dataset: $\\{0, 1, \\ldots, 20\\}$\n",
    "\n",
    "1. **Calculate the sample mean $\\hat{\\mu}$**:\n",
    "\n",
    "$$\n",
    "\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n",
    "$$\n",
    "\n",
    "Where $n = 21$ (since there are 21 numbers from 0 to 20):\n",
    "\n",
    "$$\n",
    "\\hat{\\mu} = \\frac{1}{21} \\sum_{i=0}^{20} i = \\frac{1}{21} \\cdot \\frac{20 \\cdot (20 + 1)}{2} = \\frac{1}{21} \\cdot 210 = 10\n",
    "$$\n",
    "\n",
    "2. **Calculate the sample variance $\\hat{\\sigma}^2$**:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{1}{n} \\sum_{i=1}^{n} (x_i - \\hat{\\mu})^2\n",
    "$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 = \\frac{770}{21} \\approx 36.67\n",
    "$$\n",
    "\n",
    "So, the MLE estimates are:\n",
    "\n",
    "$$\n",
    "\\hat{\\mu} = 10\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}^2 \\approx 36.67\n",
    "$$\n",
    "\n",
    "### With Prior Knowledge (Bayesian Estimation)\n",
    "\n",
    "Using the priors:\n",
    "- $\\mu \\sim \\mathcal{N}(10, 2^2)$\n",
    "- $\\sigma^2 \\sim \\text{Inverse-Gamma}(1.6505255701202999, 0.6506534459879487)$\n",
    "\n",
    "1. **Posterior for $\\mu$**:\n",
    "\n",
    "The posterior distribution for $\\mu$ given the data and $\\sigma^2$ is:\n",
    "\n",
    "$$\n",
    "\\mu | \\{x_1, \\ldots, x_n\\}, \\sigma^2 \\sim \\mathcal{N}\\left( \\frac{\\sigma_0^2 \\sum_{i=1}^{n} x_i + \\sigma^2 \\mu_0}{n \\sigma_0^2 + \\sigma^2}, \\frac{\\sigma_0^2 \\sigma^2}{n \\sigma_0^2 + \\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "Given:\n",
    "\n",
    "$$\n",
    "\\sigma_0^2 = 2^2 = 4\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mu_0 = 10\n",
    "$$\n",
    "\n",
    "\n",
    "The posterior mean is:\n",
    "\n",
    "$$\n",
    "\\mu | \\{0, 1, \\ldots, 20\\}, \\sigma^2 \\sim \\mathcal{N}\\left( \\frac{4 \\cdot 210 + 21 \\cdot \\sigma^2 \\cdot 10}{21 \\cdot 4 + 21 \\cdot \\sigma^2}, \\frac{4 \\cdot \\sigma^2}{21 \\cdot 4 + 21 \\cdot \\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "Simplifying the posterior mean:\n",
    "\n",
    "$$\n",
    "\\mu | \\{0, 1, \\ldots, 20\\}, \\sigma^2 \\sim \\mathcal{N}\\left( \\frac{840 + 210 \\sigma^2}{84 + 21 \\sigma^2}, \\frac{4 \\sigma^2}{84 + 21 \\sigma^2} \\right)\n",
    "$$\n",
    "\n",
    "2. **Posterior for $\\sigma^2$**:\n",
    "\n",
    "The posterior distribution for $\\sigma^2$ given the data and $\\mu$ is:\n",
    "\n",
    "$$\n",
    "\\sigma^2 | \\{0, 1, \\ldots, 20\\} \\sim \\text{Inverse-Gamma}\\left( \\alpha + \\frac{n}{2}, \\beta + \\frac{1}{2} \\sum_{i=0}^{20} (x_i - \\mu)^2 \\right)\n",
    "$$\n",
    "\n",
    "Given:\n",
    "$$\n",
    "\\alpha = 1.6505255701202999\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\beta = 0.6506534459879487\n",
    "$$\n",
    "\n",
    "The posterior parameters are:\n",
    "\n",
    "\n",
    "\n",
    "Thus, the posterior distribution for $\\sigma^2$ is:\n",
    "\n",
    "$$\n",
    "\\sigma^2 | \\{0, 1, \\ldots, 20\\} \\sim \\text{Inverse-Gamma}(12.1505255701202999, 385.6506534459879487)\n",
    "$$\n",
    "\n",
    "### Final Results of Bayesian Estimates\n",
    "\n",
    "- **Posterior for $\\mu$**:\n",
    "  $$\n",
    "  \\mu | \\{0, 1, \\ldots, 20\\}, \\sigma^2 \\sim \\mathcal{N}\\left( \\frac{840 + 210 \\sigma^2}{84 + 21 \\sigma^2}, \\frac{4 \\sigma^2}{84 + 21 \\sigma^2} \\right)\n",
    "  $$\n",
    "\n",
    "- **Posterior for $\\sigma^2$**:\n",
    "  $$\n",
    "  \\sigma^2 | \\{0, 1, \\ldots, 20\\} \\sim \\text{Inverse-Gamma}(12.1505255701202999, 385.6506534459879487)\n",
    "  $$\n",
    "\n",
    "In practice, to obtain point estimates from the posterior distributions, you might take the mean or mode of the posterior distributions. The mean of an inverse-gamma distribution $ \\text{Inverse-Gamma}(\\alpha, \\beta) $ is given by $ \\frac{\\beta}{\\alpha - 1} $ (for $ \\alpha > 1 $):\n",
    "\n",
    "- Posterior mean for $\\sigma^2$:\n",
    "\n",
    "  $$\n",
    "  \\mathbb{E}[\\sigma^2 | \\{0, 1, \\ldots, 20\\}] = \\frac{385.6506534459879487}{12.1505255701202999 - 1} \\approx 35.78\n",
    "  $$\n",
    "\n",
    "Thus, the Bayesian estimates are approximately:\n",
    "\n",
    "- $\\hat{\\mu} \\approx 10$ (due to the symmetric nature of the normal prior and the data)\n",
    "- $\\hat{\\sigma}^2 \\approx 35.78$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 1.6505255701202999, Beta: 0.6506534459879487\n",
      "\n",
      "Without Prior:\n",
      "Mean (μ): 10.5\n",
      "Standard Deviation (σ): 1.6035674514745464\n",
      "\n",
      "With Prior:\n",
      "Posterior Mean (μ): 16.80387931034483\n",
      "Posterior Standard Deviation (μ): 0.09847982464479192\n",
      "Posterior Standard Deviation (σ): 2.3577878503466803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dr\\AppData\\Local\\Temp\\ipykernel_13928\\1516641445.py:28: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  alpha, beta = optimize.fsolve(equations, initial_guess)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import scipy.optimize as optimize\n",
    "from scipy.stats import invgamma\n",
    "\n",
    "# Generate some example data\n",
    "data = np.array([10, 12, 11, 9, 8, 13, 10, 11])\n",
    "\n",
    "# Prior information for μ\n",
    "prior_mu_mean = 17\n",
    "prior_mu_std = 0.1\n",
    "\n",
    "# Target mean and extreme value for σ\n",
    "prior_sigma_mean = 1\n",
    "prior_sigma_max = 3\n",
    "\n",
    "# Function to solve for alpha and beta\n",
    "def equations(params):\n",
    "    alpha, beta = params\n",
    "    mean_eq = beta / (alpha - 1) - prior_sigma_mean\n",
    "    prob_eq = 1 - stats.invgamma.cdf(prior_sigma_max, alpha, scale=beta) - 0.05  # 5% extreme value\n",
    "    return [mean_eq, prob_eq]\n",
    "\n",
    "# Initial guess for alpha and beta\n",
    "initial_guess = [2, 1]\n",
    "\n",
    "# Solve the equations\n",
    "alpha, beta = optimize.fsolve(equations, initial_guess)\n",
    "\n",
    "print(f\"Alpha: {alpha}, Beta: {beta}\")\n",
    "\n",
    "# Estimate parameters without prior\n",
    "data_mean = np.mean(data)\n",
    "data_std = np.std(data, ddof=1)\n",
    "\n",
    "print(\"\\nWithout Prior:\")\n",
    "print(f\"Mean (μ): {data_mean}\")\n",
    "print(f\"Standard Deviation (σ): {data_std}\")\n",
    "\n",
    "# Estimate parameters with prior\n",
    "# For μ ~ N(prior_mu_mean, prior_mu_std^2)\n",
    "n = len(data)\n",
    "mu_posterior_mean = (prior_mu_mean / prior_mu_std**2 + n * data_mean / data_std**2) / (1 / prior_mu_std**2 + n / data_std**2)\n",
    "mu_posterior_std = np.sqrt(1 / (1 / prior_mu_std**2 + n / data_std**2))\n",
    "\n",
    "# Calculate the posterior parameters for the inverse gamma distribution\n",
    "alpha_post = alpha + n / 2\n",
    "beta_post = beta + np.sum((data - data_mean)**2) / 2\n",
    "\n",
    "# Sample from the posterior inverse gamma distribution\n",
    "sigma_posterior_samples = invgamma.rvs(alpha_post, scale=beta_post, size=10000)\n",
    "sigma_posterior = np.sqrt(np.mean(sigma_posterior_samples**2))\n",
    "\n",
    "print(\"\\nWith Prior:\")\n",
    "print(f\"Posterior Mean (μ): {mu_posterior_mean}\")\n",
    "print(f\"Posterior Standard Deviation (μ): {mu_posterior_std}\")\n",
    "print(f\"Posterior Standard Deviation (σ): {sigma_posterior}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interesting for All Students\n",
    "\n",
    "To correct or bias the grades using the calculated posterior mean (μ) and standard deviation (σ) obtained by incorporating prior information, outline of the method is:\n",
    "\n",
    "### 1. Obtain the Posterior Mean and Standard Deviation\n",
    "\n",
    "First, you calculate the posterior mean (μ) and standard deviation (σ) using prior information as outlined previously. The formulas are:\n",
    "\n",
    "- Posterior mean of μ:\n",
    "  $$\n",
    "  \\mu_{\\text{posterior}} = \\frac{\\frac{\\mu_0}{\\sigma_0^2} + \\frac{n \\cdot \\mu_{\\text{ML}}}{\\sigma_{\\text{ML}}^2}}{\\frac{1}{\\sigma_0^2} + \\frac{n}{\\sigma_{\\text{ML}}^2}}\n",
    "  $$\n",
    "\n",
    "- Posterior standard deviation of μ:\n",
    "  $$\n",
    "  \\sigma_{\\mu_{\\text{posterior}}} = \\sqrt{\\frac{1}{\\frac{1}{\\sigma_0^2} + \\frac{n}{\\sigma_{\\text{ML}}^2}}}\n",
    "  $$\n",
    "\n",
    "### 2. Correct the Data\n",
    "\n",
    "To bias or correct the data, you need to adjust each data point based on the posterior mean and standard deviation. The adjustment involves shifting and scaling the data points.\n",
    "\n",
    "### 3. Correction Formula\n",
    "\n",
    "Given an original data point $x_i$, the corrected data point $x_i'$ can be calculated as follows:\n",
    "\n",
    "$$\n",
    "x_i' = \\mu_{\\text{posterior}} + \\frac{x_i - \\mu_{\\text{ML}}}{\\sigma_{\\text{ML}}} \\cdot \\sigma_{\\text{posterior}}\n",
    "$$\n",
    "\n",
    "Simple code is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dr\\AppData\\Local\\Temp\\ipykernel_13928\\2875037383.py:28: RuntimeWarning: The iteration is not making good progress, as measured by the \n",
      "  improvement from the last ten iterations.\n",
      "  alpha, beta = optimize.fsolve(equations, initial_guess)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 1.6505255701202999, Beta: 0.6506534459879487\n",
      "\n",
      "Without Prior:\n",
      "Mean (μ): 10.5\n",
      "Standard Deviation (σ): 1.6035674514745464\n",
      "\n",
      "With Prior:\n",
      "Posterior Mean (μ): 12.081081081081082\n",
      "Posterior Standard Deviation (μ): 0.4931969619160719\n",
      "Posterior Standard Deviation (σ): 2.3378232105711434\n",
      "\n",
      "Corrected Data:\n",
      "[11.35213662 14.26791445 12.81002554  9.89424771  8.4363588  15.72580337\n",
      " 11.35213662 12.81002554]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import scipy.optimize as optimize\n",
    "from scipy.stats import invgamma\n",
    "\n",
    "# Generate some example data\n",
    "data = np.array([10, 12, 11, 9, 8, 13, 10, 11])\n",
    "\n",
    "# Prior information for μ\n",
    "prior_mu_mean = 17\n",
    "prior_mu_std = 1\n",
    "\n",
    "# Target mean and extreme value for σ\n",
    "prior_sigma_mean = 1\n",
    "prior_sigma_max = 3\n",
    "\n",
    "# Function to solve for alpha and beta\n",
    "def equations(params):\n",
    "    alpha, beta = params\n",
    "    mean_eq = beta / (alpha - 1) - prior_sigma_mean\n",
    "    prob_eq = 1 - stats.invgamma.cdf(prior_sigma_max, alpha, scale=beta) - 0.05  # 5% extreme value\n",
    "    return [mean_eq, prob_eq]\n",
    "\n",
    "# Initial guess for alpha and beta\n",
    "initial_guess = [2, 1]\n",
    "\n",
    "# Solve the equations\n",
    "alpha, beta = optimize.fsolve(equations, initial_guess)\n",
    "\n",
    "print(f\"Alpha: {alpha}, Beta: {beta}\")\n",
    "\n",
    "# Estimate parameters without prior\n",
    "data_mean = np.mean(data)\n",
    "data_std = np.std(data, ddof=1)\n",
    "\n",
    "print(\"\\nWithout Prior:\")\n",
    "print(f\"Mean (μ): {data_mean}\")\n",
    "print(f\"Standard Deviation (σ): {data_std}\")\n",
    "\n",
    "# Estimate parameters with prior\n",
    "# For μ ~ N(prior_mu_mean, prior_mu_std^2)\n",
    "n = len(data)\n",
    "mu_posterior_mean = (prior_mu_mean / prior_mu_std**2 + n * data_mean / data_std**2) / (1 / prior_mu_std**2 + n / data_std**2)\n",
    "mu_posterior_std = np.sqrt(1 / (1 / prior_mu_std**2 + n / data_std**2))\n",
    "\n",
    "# Calculate the posterior parameters for the inverse gamma distribution\n",
    "alpha_post = alpha + n / 2\n",
    "beta_post = beta + np.sum((data - data_mean)**2) / 2\n",
    "\n",
    "# Sample from the posterior inverse gamma distribution\n",
    "sigma_posterior_samples = invgamma.rvs(alpha_post, scale=beta_post, size=10000)\n",
    "sigma_posterior = np.sqrt(np.mean(sigma_posterior_samples**2))\n",
    "\n",
    "print(\"\\nWith Prior:\")\n",
    "print(f\"Posterior Mean (μ): {mu_posterior_mean}\")\n",
    "print(f\"Posterior Standard Deviation (μ): {mu_posterior_std}\")\n",
    "print(f\"Posterior Standard Deviation (σ): {sigma_posterior}\")\n",
    "\n",
    "# Correct the data using the posterior mean and standard deviation\n",
    "corrected_data = mu_posterior_mean + (data - data_mean) / data_std * sigma_posterior\n",
    "\n",
    "print(\"\\nCorrected Data:\")\n",
    "print(corrected_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Experiments\n",
    "\n",
    "**Main Data:**\n",
    "\n",
    "$$\n",
    "10, 12, 11, 9, 8, 13, 10, 11\n",
    "$$\n",
    "\n",
    "**Prior Information for $ \\mu $:**\n",
    "- prior_mu_mean = 17\n",
    "- prior_mu_std = 0.1\n",
    "\n",
    "**Corrected Data:**\n",
    "$$\n",
    "16.07820141, 18.98091301, 17.52955721, 14.62684561, 13.17548981, 20.43226882, 16.07820141, 17.52955721\n",
    "$$\n",
    "\n",
    "**Prior Information for μ:**\n",
    "- prior_mu_mean = 17\n",
    "- prior_mu_std = 1\n",
    "\n",
    "**Corrected Data:**\n",
    "$$\n",
    "11.35213662, 14.26791445, 12.81002554, 9.89424771, 8.4363588, 15.72580337, 11.35213662, 12.81002554\n",
    "$$\n",
    "\n",
    "These experiments demonstrate that the certainty of the prior shifts all data towards the mean of the prior. If the prior has uncertainty, the main data returns to its original values."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".M_HomePage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
